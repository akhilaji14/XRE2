---
layout: page
title: "E2. AR Foundation"
---

> # Pre-Class Prep  
> **Estimated Prep Time:** 45–55 minutes  
> - **[What Is AR Foundation?](https://github.gatech.edu/mmoghaddam3/XRE/wiki/E2.-AR-Foundation#what-is-ar-foundation):** Read wiki's AR Foundation overview to understand the role of AR Foundation and how it enables cross-platform AR development. Focus on the idea of abstraction and note which platforms (e.g., ARKit, ARCore, OpenXR) are supported so you are ready to discuss deployment strategy and device compatibility in class.
> - **[Setting Up an AR Project](https://github.gatech.edu/mmoghaddam3/XRE/wiki/E2.-AR-Foundation#setting-up-an-ar-project):** Open Unity and create a new 3D project. In the `Package Manager`, locate the `AR Foundation` package and at least one provider plug-in for your device (`ARCore`, `ARKit`, or `OpenXR`). Also review the steps to configure URP (via Unity Docs or a short tutorial video) so you are familiar with how to set up rendering for AR scenes. Try to deploy a basic app (e.g., a cube) into your smartphone, if possible.
> - **[Architecture](https://github.gatech.edu/mmoghaddam3/XRE/wiki/E2.-AR-Foundation#architecture):** Skim Unity’s AR Subsystems documentation with a focus on understanding key architectural elements: **subsystems**, **descriptors**, and **managers**. Grasping these concepts will help you follow the in-class walkthrough on how Unity standardizes AR functionality across devices while letting you manage real-world trackables as GameObjects.

---

# What Is AR Foundation?

AR Foundation is a high-level framework developed by Unity that allows you to create AR experiences that run seamlessly across multiple AR platforms. Rather than working directly with platform-specific SDKs like **[ARKit](https://developer.apple.com/augmented-reality/arkit/)** (Apple) or **[ARCore](https://developers.google.com/ar)** (Google), AR Foundation provides a unified API and a set of manager components that handle the underlying AR functionality. These components include session tracking, plane detection, anchors, environment probes, light estimation, and more.  When deployed, AR Foundation automatically connects your Unity code to the appropriate native AR SDK, abstracting away the differences between platforms while giving you access to platform-specific features when needed. AR Foundation supports a wide array of AR hardware platforms, including:

- **iOS/iPadOS devices** with ARKit  
- **Android phones/tablets** with ARCore  
- **Magic Leap 2** headset (via Unity’s OpenXR support or Magic Leap’s XR Plug-in)  
- **Microsoft HoloLens 2** headset (via OpenXR)  
- **Meta Quest and Meta Quest Pro** headsets (via Unity OpenXR for Meta)  
- **Apple visionOS** devices (e.g., Apple Vision Pro headset)  

> This broad support makes AR Foundation a powerful tool for creating immersive AR experiences across both mobile and head-worn devices used in engineering, manufacturing, healthcare, education, and beyond.

## Why AR Foundation?

- **Multi-Platform Deployment:** Write your app once in Unity and deploy it to a range of AR devices, from smartphones to advanced headsets. This shared codebase approach dramatically reduces development time and streamlines updates.

- **Simplified Development:** AR Foundation unifies the AR workflow by abstracting platform differences. Developers can focus on designing compelling user experiences instead of managing low-level device-specific logic.

- **Extensibility:** AR Foundation plays well with Unity’s broader ecosystem—including XR Interaction Toolkit, Shader Graph, and URP/HDRP—allowing you to extend functionality and tailor experiences to different hardware capabilities.

- **Access to Native Features:** While AR Foundation simplifies development, it also allows access to underlying platform-specific features through subsystem extensions or conditional logic, giving you full control when needed.

- **OpenXR Compatibility:** Support for OpenXR makes AR Foundation future-proof and aligned with industry standards for XR development, easing the integration of new devices as they emerge.

- **Toolchain Integration:** Unity’s integration with popular version control systems, CI/CD pipelines, and asset management tools makes AR Foundation projects easy to collaborate on in large teams.

- **Rapid Prototyping:** Unity's real-time editor combined with AR Foundation enables fast iteration cycles, making it easier to test, debug, and refine AR interactions early in the development process.

- **Active Ecosystem & Support:** Backed by Unity’s documentation, tutorials, and community support, AR Foundation benefits from continuous updates and a robust knowledge base for troubleshooting and learning.


## Required Packages

AR Foundation's **provider plug-in packages** provide a unified interface to access AR features. This package is central to your AR project as it defines the common APIs used to interact with various AR functionalities. Each plug-in is essential for enabling AR capabilities on its corresponding platform. This modular approach allows developers to mix and match components based on target devices and feature requirements.

- **Apple ARKit XR Plug-in (iOS):** Enables AR functionalities for iPhones and iPads.
- **Google ARCore XR Plug-in (Android):** Implements AR features on Android smartphones and tablets.
- **Magic Leap XR Plug-in (Magic Leap 2):** Enables native AR capabilities specific to Magic Leap 2 devices, including spatial mapping, hand tracking, and eye tracking. Can also be supported via Unity OpenXR.
- **OpenXR Plug-in (HoloLens 2):** Provides AR support for Microsoft HoloLens 2 via the OpenXR standard.
- **Apple visionOS XR Plug-in (visionOS):** Supports AR on Apple’s visionOS platform, including Apple Vision Pro.
- **Unity OpenXR: Meta (Meta Quest):** Dedicated to AR experiences on Meta Quest and Meta Quest Pro headsets using passthrough AR and spatial anchors.
- **Unity OpenXR: Android XR (Android XR):** Delivers AR support on Android-based XR devices such as enterprise-focused smart glasses.

> AR Foundation does not implement AR features by itself. It requires additional provider plug-in packages. Without these platform-specific extensions, AR Foundation would not be able to leverage the native capabilities of each AR-enabled device.


## Key Features

Several key features are provided by AR Foundation, which establish a robust framework that caters to simple AR demonstrations as well as complex, interactive, and multi-user experiences, forming the essential building blocks for compelling AR applications.

- **[Session](https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@6.1/manual/features/session.html):** Manages the AR experience by enabling, disabling, and configuring AR sessions on the device. This feature ensures that the AR experience is initialized correctly and can be dynamically managed during runtime.
- **[Device Tracking](https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@6.1/manual/features/device-tracking.html):** Tracks the position and orientation of the device within the physical space, which is crucial for aligning virtual elements with the real world.
- **[Camera](https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@6.1/manual/features/camera.html):** Renders images from the device’s camera while providing light estimation to integrate virtual content with real-world lighting conditions, helping virtual objects to appear natural within the scene.
- **[Plane Detection](https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@6.1/manual/features/plane-detection.html):** Identifies flat surfaces such as floors, tables, or walls in the environment, enabling precise placement of virtual content on detected surfaces.
- **[Bounding Box Detection](https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@6.1/manual/features/bounding-box-detection.html):** Detects and tracks bounding boxes around 3D objects, which facilitates interactive object manipulation.
- **[Image Tracking](https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@6.1/manual/features/image-tracking.html):** Recognizes and tracks 2D images in the real world, allowing the app to trigger AR content or interactive experiences based on visual cues.
- **[Object Tracking](https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@6.1/manual/features/object-tracking.html):** Recognizes and tracks physical 3D objects, creating opportunities for interactive experiences rooted in object recognition.
- **[Face Tracking](https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@6.1/manual/features/face-tracking.html):** Detects and monitors human faces for expressions and positioning, enabling the creation of personalized, context-aware AR experiences.
- **[Body Tracking](https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@6.1/api/UnityEngine.XR.ARFoundation.ARHumanBodyManager.html):** Captures and follows the pose and movement of a human body, allowing for full-body interactive engagements in AR.
- **[Point Clouds](https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@6.1/manual/features/point-clouds.html):**  Detect a set of feature points in the environment that help in mapping spatial structures and enhancing the accuracy of AR scene reconstruction.
- **[Ray Casts](https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@6.1/manual/features/raycasts.html):** Cast virtual rays to interact with detected surfaces and objects, supporting intuitive user interactions, such as selecting or placing virtual items.
- **[Anchors](https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@6.1/manual/features/anchors.html):** Attach virtual objects to fixed points in the physical world to maintain consistent positioning as the user moves, ensuring a persistent AR experience.
- **[Meshing](https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@6.1/manual/features/meshing.html):** Generates a mesh representation of the physical environment, which can be used to create occlusion effects or provide a dynamic surface model for interactions.
- **[Environment Probes](https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@6.1/manual/features/environment-probes.html):** Capture environmental lighting and reflections by generating cubemaps, ensuring that virtual objects reflect the real-world lighting accurately.
- **[Occlusion](https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@6.1/manual/features/occlusion.html):** Renders virtual content behind physical objects using depth and human segmentation data, enhancing realism by properly layering virtual and real elements.
- **[Participants](https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@6.1/manual/features/participant-tracking.html):** Support multi-user AR experiences by tracking other devices in a shared session, which enables collaborative and social AR applications.


## Platform Support

AR Foundation uses native AR implementations from various platforms. However, not every AR feature is uniformly available across all platforms. For example, while basic session management and plane detection are widely supported, advanced features like object tracking, meshing, or eye tracking might be limited to certain devices. Specific platforms that are supported include:

- **Apple visionOS (e.g., Apple Vision Pro):** Provides spatial computing capabilities with advanced features such as volumetric scene understanding, object occlusion, high-fidelity passthrough, and eye tracking, tailored to Apple’s head-mounted AR hardware.

- **ARCore (Android) and ARKit (iOS):** These core providers offer broad support for essential AR features such as session management, plane detection, light estimation, and image tracking. ARCore supports a wide range of Android smartphones and tablets, while ARKit is built into iOS and iPadOS devices.

- **Magic Leap 2:** Offers advanced AR capabilities such as world meshing, hand tracking, spatial anchors, controller input, and eye tracking. Magic Leap 2 can be used with its dedicated Unity XR Plug-in or through OpenXR support.

- **Microsoft HoloLens 2 (via OpenXR):** Supports enterprise-grade AR features including spatial mapping, articulated hand tracking, eye tracking, spatial anchors, and voice input, with seamless integration using Unity’s OpenXR framework.

- **Meta Quest and Meta Quest Pro (via OpenXR):** Enable passthrough AR experiences using external cameras, in addition to support for spatial anchors, hand tracking, and basic meshing. Unity OpenXR extensions for Meta devices allow AR development beyond traditional VR.

- **Android XR (OpenXR):** Supports AR functionalities on XR-enabled Android-based smart glasses and headsets used in industrial and enterprise environments. Feature support varies by device manufacturer.

- **XR Simulation (Unity Editor):** Many AR features—such as plane detection, anchors, and image tracking—can be simulated and tested directly within the Unity Editor using the XR Simulation feature. This allows developers to prototype and debug AR experiences without needing a physical AR device for every test.

> When planning an AR project, it is crucial to verify feature availability and compatibility for your target platform(s) and to install the corresponding provider plug-ins. Review Unity's AR Foundation documentation and the manufacturer’s SDKs to ensure your application behaves as expected on each supported device.


---

# Setting Up an AR Project

Now, let's configure a Unity project for AR development using AR Foundation. This involves installing required packages and settings, setting up the Universal Render Pipeline (URP), creating an AR-ready scene, and deploying your project to an iOS or Android device. To make this concrete, let's bring the virtual drone from XFactory into the (physical) room via AR.

## Install AR Foundation

AR Foundation is available via the Unity `Package Manager` and provides the common interfaces to access AR features. This package allows you to develop AR apps in a standardized way across multiple devices. It acts as the core framework to which platform-specific providers are attached. To install:

1. Create a new project using the `Universal 3D` template. Name it `XFactoryAR`. Click `+ Create project` to open it.
2. Open Unity Editor and navigate to `Window > Package Manager`.
3. In the `Package Manager`, locate and install `AR Foundation` in `Unity Registry`. 

![01](/Figures/E2/01.jpg)

> Installing AR Foundation is critical for building the basic AR framework and functionalities in your project.


## Install Provider Plug-ins

The provider plug-ins implement AR features for specific target platforms. Each plug-in bridges AR Foundation calls to the native AR SDK of the device, ensuring compatibility and optimal performance. They are essential for unlocking the full functionality of AR on each respective platform. Some plug-ins require extra project configuration to enable optimal performance and functionality. Proper project settings ensure that all AR features work seamlessly across devices. It also helps prevent runtime issues by aligning project configurations with each provider’s requirements. To install:

1. Navigate to `Edit > Project Settings > XR Plug-in Management`.
2. If prompted, click `Install XR Plug-in Management`.
3. Once installed, choose your **target platform** tab. If **iOS**, check `ARKit`. If **Android**, check `ARCore`.
4. Unity will now automatically install the required provider packages: `ARKit XR Plugin` for iOS or `ARCore XR Plugin` for Android.

![02](/Figures/E2/02.jpg)

## Set Up an AR Scene

Correctly setting up your AR scene ensures that virtual objects align with the physical world, that tracking works reliably, and that AR content is rendered from the device’s perspective.

1. **Start with a Clean Scene:**
  - Delete the default `Main Camera`. This is replaced by the camera inside the `XR Origin` prefab.
  - Keep or customize the default `Directional Light`. Delete it only if you plan to use environmental lighting or AR light estimation.
  - Delete the default `Global Volume` (if present), unless you are intentionally using post-processing effects compatible with URP and AR. These can often interfere or add unnecessary complexity in early AR prototyping.
  - Save it as `AR101` scene.

    ![07](/Figures/E2/07.jpg)

2. **Add `AR Session`:**
  - In the `Hierarchy`, right-click and go to `XR > AR Session`. This creates the `AR Session` object, which initializes and manages the AR system. It is essential for tracking and accessing features like plane detection and light estimation using various managers (components).

    ![08](/Figures/E2/08.jpg)

3. **Add `XR Origin (Mobile AR)`:**
  - Right-click again and select `XR > XR Origin (Mobile AR)`. This adds the `XR Origin` prefab, which includes an AR-compatible camera and the proper spatial hierarchy to reflect the user's device in the virtual space.
  - Confirm the `XR Origin` structure. It should contain `Camera Offset > Main Camera` (tagged as `MainCamera`).

    ![09](/Figures/E2/09.jpg)

    > This `Main Camera` under `XR Origin` will automatically follow the **device's position and orientation** in the real world.

4. **Configure AR Camera Settings:**
  - Select the `Main Camera` inside the `XR Origin`.
  - Locate the `AR Camera Manager` component.
  - Enable `Auto Focus` to keep the physical camera lens focused automatically. Useful for dynamic scenes or close-up content.
  - Choose an appropriate `Light Estimation` setting. For example, `None` uses no light data from the environment, `Ambient Intensity` or `Ambient Spherical Harmonics` enable virtual objects to reflect real-world lighting conditions, and `Environmental HDR` (if supported) enables realistic lighting and reflections, but may reduce performance on lower-end devices.
  - Set `Facing Direction` to `World` (back camera) for most AR use cases. Use `User` (front camera) only for face-tracking or selfie-style AR experiences.
  - Set `Render Mode` to `Before Opaques` unless you have a custom render ordering need. This mode ensures the camera feed renders before your AR content.
  - Also, on the `Main Camera`, configure the `AR Camera Background` component.
  - Leave `Use Custom Material` unchecked for most cases. AR Foundation automatically uses a platform-appropriate shader to render the camera feed. Check this only if you're applying a custom shader/material (e.g., to add visual effects like color tint, distortion, or depth blending).

    ![12](/Figures/E2/12.jpg)

5. **Import Custom Assets:**
  - Drag and drop the `XFactoryAssets.unitypackage` into your `Project` window or import them via `Assets > Import Package > Custom Package...`.
  - In the import dialog, make sure all items are selected and click `Import`.

    ![10](/Figures/E2/10.jpg)

6. **Place the Drone into the Scene:**
  - Drag the `Drone` prefab from `Assets > XFactory > Prefabs > Drone` into the `Hierarchy` as a top-level GameObject.
  - Select the `Drone` GameObject in the `Scene` view and position it approximately **1.5 meters in front of the camera** (e.g., `Position = (0, 0, 1.5)`).

    ![11](/Figures/E2/11.jpg)

    > Since the drone is placed in **world space**, it will appear in front of the user at app launch and remain fixed in place as the user moves around. This setup gives you immediate visibility for testing without the drone following the device.

7. **Locate the URP Asset:**
  - Open `Edit > Project Settings > Graphics`.
  - Under `Default Render Pipeline`, click the asset selector and choose `Mobile_RPAsset`.

    ![03](/Figures/E2/03.jpg)

8. **Configure URP for AR:**
  - Go to your `Project` window, locate the asset named `Mobile_Renderer` (usually found under `Assets/Settings/`)  
  - Select it to open its `Inspector`.
  - Scroll down to `Renderer Features`.
  - Click `Add Renderer Feature`.
  - Choose `AR Background Renderer Feature`.

    ![04](/Figures/E2/04.jpg)

> AR Foundation supports both the **Built-in Render Pipeline** and the **Universal Render Pipeline (URP)**. If you're using URP, Unity 6 already includes default render pipeline assets you can use for AR — no need to create custom ones.


## Deploying to iOS

Testing your AR application on an iOS device is crucial because it offers authentic feedback on performance, user interface behavior, and compatibility with Apple’s hardware and software ecosystem. Before you proceed, download and install **Xcode** from the Mac App Store. Xcode is required to compile and deploy your Unity project on iOS devices, making it an essential tool for iOS development.

![13](/Figures/E2/13.jpg)


1. **Enable `ARKit`:**
  - Go to `Edit > Project Settings > XR Plug-in Management`.
  - Select the `iOS` tab and ensure that `ARKit` is enabled. Enabling ARKit connects your project to Apple’s native AR framework, allowing you to access advanced AR functionalities on iOS devices.

2. **Configure Bundle Identifier:**
  - Open `Edit > Project Settings > Player`.
  - Set the `Company Name` and `Product Name`. This step generates a unique `Bundle Identifier`, which is essential for app identification and deployment on the Apple ecosystem.
  - Ensure that the `Bundle Identifier` (formatted as `com.CompanyName.ProductName`) is unique and free of spaces or special characters. A correct identifier prevents build errors and is required for distribution on the App Store.
  - Scroll down to `Camera Usage Description` and enter a brief explanation This will be shown to users the first time the app requests camera access.

    ![14](/Figures/E2/14.jpg)

3. **Configure Build Settings:**
  - Navigate to `File > Build Profiles`.
  - Go to `Scene List`, click the `Add Open Scenes` button, and only check `Scenes/AR101`. Adding your current scene ensures that it is included in the build.
  - Select `iOS` as your target platform and click `Switch Platform`. Switching the platform configures your project with iOS-specific libraries and settings.
  - Click `Build` to create an Xcode project. This exports your Unity project into an Xcode project, which you can then compile and deploy to an iOS device.

    ![15](/Figures/E2/15.jpg)

4. **Build to Device Using Xcode:**
  - Open the generated Xcode project (`Unity-iPhone.xcodeproj`). 
  - Connect your iOS device via cable. 
  - Resolve any signing issues. Enable `Automatically manage signing` and assign your team under `Signing & Capabilities`. Resolving signing issues ensures that your app is properly authenticated and trusted by the iOS system.
  - Once signing issues are resolved, select your device as the run destination and click the run button in Xcode. Running the project on your device lets you test AR functionality in real conditions.
  - Trust the developer profile on your iOS device if prompted (navigate to `Settings > General > VPN & Device Management`). Trusting the profile allows your device to run the app without security blocks.

    ![16](/Figures/E2/16.jpg)

5. **Test on Your iOS Device:**
  - On your iOS device, select `Enable Developer Mode`.
  - When launched, you will be prompted for camera access. 
  - Once camera access is granted, the app will show your real-world environment as the background, with the drone hovering in front of you, anchored in world space.

    ![21](/Figures/E2/21.jpg)


## Deploying to Android

Testing your AR application on an Android device is vital since it provides accurate insights into performance, adaptability, and the overall user experience on a wide range of Android hardware configurations. Before you proceed, ensure the **Android Build Support** module is installed with Unity. This module is necessary to compile your project into an APK file for Android devices. On your Android device, enable USB Debugging from the Developer Options menu. USB Debugging enables communication between your Android device and the Unity Editor for testing purposes.

![17](/Figures/E2/17.jpg)

1. **Enable `ARCore`:**
  - Go to `Edit > Project Settings > XR Plug-in Management`.
  - Select the `Android` tab and ensure that the `ARCore` plug-in is enabled. Enabling `ARCore` connects your project to Google's AR platform, providing access to device-specific AR features.
  - Go to `Project Validation` and fix all the issues.

    ![18](/Figures/E2/18.jpg)

2. **Configure Build Profiles:**
  - Navigate to `File > Build Profiles`.
  - Add the open scene(s) with `Add Open Scenes`. This ensures that the scene you are working on will be part of the final APK.
  - Select `Android` and click `Switch Platform`. This step converts your project for Android, applying the necessary settings and libraries.

    ![19](/Figures/E2/19.jpg)

3. **Deploy Your App:**
  - Connect your Android device via cable.
  - In the `Build Profiles` window, click `Refresh` to list your connected device.
  - Click `Build and Run`. Save your build as an APK file (for example, in a folder named `Builds`). This creates an APK, the standard format for Android applications, allowing you to install and test your AR app.
  - Once the app installs and runs on your device, grant any necessary permissions (such as camera access). Granting permissions ensures that the app can access essential hardware and run correctly.

    ![20](/Figures/E2/20.jpg)

> Iterate by making small modifications (for instance, adjust the scale of a GameObject) to confirm that updated builds deploy correctly. Iterative testing is key to identifying and resolving issues quickly. Configure your Game view’s aspect ratio (for example, set it to 16:9 Portrait) within Unity to accurately simulate the mobile device experience during development. This allows you to preview and adjust how your app will appear on actual devices.


---

# Architecture

Understanding the AR Foundation architecture is key to making informed decisions when building your AR application. Software architecture refers to the overall structure of a software system. In the context of AR Foundation, this architecture is designed to abstract platform-specific implementations into a unified interface, enabling you to develop AR apps across multiple devices with a consistent API. By understanding these elements, you can better design your application, optimize performance, and troubleshoot issues when integrating native AR capabilities.

> For engineers, this knowledge is especially useful when building scalable, efficient AR solutions for domains like robotics, control systems, industrial inspection, or digital twins—where reliability and cross-platform deployment are critical.


## Subsystems

Subsystems (or `SubsystemWithProvider`) define the core features and life cycle of AR-related functionality. They provide an abstraction layer over platform-specific SDKs such as Google ARCore or Apple ARKit. This abstraction allows you to write common application code that interacts with multiple platforms seamlessly, reducing development complexity.

- **Life Cycle of Subsystems:** All subsystems share the same life cycle. This predictable life cycle, managed by Unity's active XRLoader, ensures consistency when initiating or terminating AR processes. It also helps in managing system resources effectively:
   - **Created:** The subsystem is instantiated.
   - **Started:** It begins processing and delivers data.
   - **Stopped:** Processing is halted.
   - **Destroyed:** The subsystem is disposed of.

   > This mirrors life cycle concepts in embedded systems or robotics where precise resource management is essential to prevent system overload or instability.

- **Subsystem Descriptors:** Each subsystem has a corresponding descriptor that outlines its capabilities and limitations (for example, whether an image tracking subsystem supports mutable libraries). Descriptors allow you to query capabilities at runtime, which is essential for creating adaptive and robust AR experiences that check for feature support on the target device.

   > This could be used to detect whether a headset supports spatial mesh reconstruction before enabling a collision avoidance feature for a robot visualization.

- **Tracking Subsystems:** A tracking subsystem is responsible for detecting and continuously tracking objects (known as trackables) in the physical environment.  
   - **Trackables:** These are entities such as planes, images, or points that are detected and monitored by the subsystem. Each trackable is uniquely identified using a 128-bit GUID, ensuring consistent tracking across frames.
   > Consider the `XRPlaneSubsystem`, which is responsible for plane detection. Whether your AR app runs on an Android or iOS device, the same code can be used to detect flat surfaces like floors or tables.

   - **How It Works:** Your app calls methods defined in the `XRPlaneSubsystem` interface to start, update, or stop plane detection. The underlying provider (ARCore for Android or ARKit for iOS) implements these methods, so your application code remains unchanged across platforms.
   - **Why It Matters:** This unified interface saves development time and minimizes errors since you don’t have to write separate implementations for each platform.

   > For example, you might want to detect workbenches in a smart factory for placing digital controls or instructions. By using `XRPlaneSubsystem`, your AR app will automatically adapt to either an iPad running ARKit or a HoloLens 2 running OpenXR—making your industrial tool more portable and scalable.


## Managers

Managers in AR Foundation are components that make subsystem data accessible to your scene as GameObjects and components. They also handle the life cycle of their respective subsystems. Managers simplify your workflow by abstracting the low-level details of subsystem management, allowing you to focus on application logic and user interaction. To add managers to the scene, select the `XR Origin` and click the `Add Component` button in the `Inspector` and search for the specific manager (e.g., `AR Plane Manager`, `AR Tracked Image Manager`). This process creates the necessary components to interface with the underlying AR subsystems, enabling features like plane detection or image tracking.

> This is especially useful because it lets you treat real-world features (like surfaces, machines, or tracked parts) **as interactive GameObjects that you can program, visualize, or annotate**—without manually handling the raw tracking data.


- **Trackable Managers:** Trackable managers wrap tracking subsystems, translating raw tracking data into usable components in your scene. For instance, `ARPlaneManager` creates and manages `ARPlane` components when the `XRPlaneSubsystem` detects a physical surface.

- **Trackable GameObjects:** These are instantiated as children of the `XR Origin`, and their positions, rotations, and scales are defined relative to that origin. This setup provides a clear, organized way to work with dynamic AR content, ensuring that virtual elements align correctly with the physical environment.

- **Configuring a Trackable's GameObject:** When a new trackable is detected, the manager instantiates a prefab (if specified) or creates an empty GameObject with the corresponding ARTrackable component.

   - **Prefab Assignment:** If a prefab is set in the manager’s `Inspector`, it will be instantiated for each detected trackable.
   - **Automatic Component Addition:** If no prefab is specified, an empty GameObject is created, and the appropriate `ARTrackable` component is added automatically. This mechanism guarantees that every detected trackable has a visual and interactive representation in the scene. It also makes it easy to customize the appearance and behavior of trackables by editing the prefab.

   > For instance, you might configure an `AR Tracked Image Manager` to recognize labels on physical machines. Each time a label is detected, a prefab UI panel could appear showing live sensor data from that machine.

- **Enabling and Disabling Managers:** Managers can be enabled or disabled to control which AR features are active. When a manager is enabled, it starts its subsystem; when disabled, it stops the subsystem. This flexibility is important for managing power consumption and processing resources, as you can turn off unused features to optimize performance.

**Example:** Imagine you want to place a floating drone in your AR application that hovers in mid-air and maintains its position as the user moves around.

- **How It Works:** You add an `AR Anchor Manager` component to your `XR Origin`. When the user taps the screen, your app performs a raycast against the AR environment (typically planes or feature points). If a valid point is found, an `ARAnchor` is created at that position, and the drone prefab is instantiated as a child of that anchor. The drone prefab includes animation for spinning propellers and a smooth hover motion, giving the illusion of autonomous flight in the room.

- **Why It Matters:** This example demonstrates how managers like the `AR Anchor Manager` simplify persistent content placement. The drone remains fixed in the real world—even as the user walks around or reorients the device—thanks to the spatial tracking capabilities of the underlying AR subsystem.

> In an industrial use case, this floating drone could simulate an autonomous inspection UAV. Users can experiment with virtual flight paths, object avoidance, or sensor visualization anchored in the actual factory space, without relying on surface detection.