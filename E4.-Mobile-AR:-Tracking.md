---
layout: page
title: "E4 Mobile AR: Tracking"
---

> # Pre-Class Prep  
> **Estimated Prep Time:** 45–55 minutes  
> - **[What Is Image Tracking](https://github.gatech.edu/mmoghaddam3/XRE/wiki/E4.-Mobile-AR:-Tracking#what-is-image-tracking):**  Read the overview on how AR Foundation detects and tracks 2D reference images in real space. Then, find or print a sample image (e.g., a label, logo, or object photo) that you'll use in Unity. Explore how this printed image could trigger an AR response—what would appear, how it would behave, and what would happen if tracking is lost.
> - **[Reference Image Library](https://github.gatech.edu/mmoghaddam3/XRE/wiki/E4.-Mobile-AR:-Tracking#reference-image-library):** In Unity, create a `ReferenceImageLibrary` using your sample image. Import the image into the `Assets` folder, add it to the library, and specify its physical width in meters (e.g., 0.2m). Verify in the Inspector that your image has visible features like edges or corners. Bonus: Experiment with adding a second image and observe the feature richness difference.
> - **[AR Tracked Image Manager](https://github.gatech.edu/mmoghaddam3/XRE/wiki/E4.-Mobile-AR:-Tracking#ar-tracked-image-manager):**  Open your Unity scene and add an `ARTrackedImageManager` to the `AR Session Origin`. Assign your library and create a placeholder prefab (e.g., a cube or text object). Set this prefab as the `Tracked Image Prefab` and preview how it appears when your image is recognized. No scripting yet—just make sure the prefab spawns.
> - **[AR Tracked Image](https://github.gatech.edu/mmoghaddam3/XRE/wiki/E4.-Mobile-AR:-Tracking#artrackedimage):** Learn the three tracking states (`None`, `Limited`, `Tracking`) and what they imply. Add basic logging (or a UI label) that changes color or text when the tracking state changes in real time. Use `ARTrackedImage.trackingState` to reflect this in the scene. Think about how your app should behave if tracking is temporarily lost.

---

# What Is Image Tracking?

Image tracking is a core feature in Unity's AR Foundation framework that enables applications to recognize, locate, and continuously track 2D images (called **reference images**) in the user’s environment. This functionality is built on top of platform-specific capabilities (like **ARKit** and **ARCore**) via the `XRImageTrackingSubsystem`. The process consists of three key functions:

- **Detect:** The image is matched against a predefined library of known images.
- **Track:** The AR system continuously estimates the position and orientation (pose) of the image in 3D space, even as the user moves.
- **Notify:** Your app is alerted through lifecycle events so it can respond dynamically—by spawning, moving, or removing virtual content bound to the image’s pose.

> Consider an AR application that allows you to simulate and control a virtual robot model superimposed on its exact physical replica. A printed image placed near the robot's real-world counterpart serves as the **reference image**. When the AR app detects this image, it overlays a virtual interface, allowing the user to trigger animations (e.g., pickup, placement, returning home).


## Principles of Image Tracking

Image tracking offers seamless integration of physical and digital content through the precise alignment of digital elements within the physical world. To understand how image tracking works and how to apply it effectively, it is helpful to grasp the underlying principles:

- **Image Recognition:** A reference image must have **high contrast**, **distinct visual features**, and **clarity**. Feature points (like corners or edges) are extracted and compared to the camera feed using computer vision algorithms.

- **Pose Estimation:** Once a reference image is recognized, the system computes its **6 degrees of freedom (6DoF) pose**—its position and orientation in 3D space relative to the device. This enables the precise placement of holograms anchored to the physical image.

- **Tracking Lifecycle:** Images are continuously evaluated in three stages, including when the image was first detected (**added**), when its pose or tracking status has changed (**updated**), and when the image is no longer visible to the camera or tracking quality is lost (**Removed**).

- **Size and Scale:** Providing a **physical width** (in meters) for each image helps improve accuracy and scale estimation. Without it, the system must guess, which may cause virtual content to misalign or scale incorrectly.

- **Environmental Factors:** Lighting, angle, occlusion, and image quality all affect tracking reliability. The image should be unobstructed, well-lit, and printed clearly.

> To register large industrial models like machines or robots, ensure the printed reference image is matte-finished, sufficiently large (~20 cm wide), and placed on a flat, stable surface near the physical replica or its representation. This ensures robust detection and reliable tracking even if users move around the environment.


## Why Use Image Tracking?

Image tracking unlocks powerful and intuitive AR experiences by tying virtual content to specific real-world images. Not only does it enhance learning and interactivity, but it also bridges the gap between virtual simulations and physical industrial environments, supporting productivity, safety, and innovation across sectors. Common applications include:

- **Contextual Experiences:** Enhance content based on what the user is looking at.  
   > Pointing a tablet or AR device at a marker placed near a physical robot can bring up an interactive control panel. Users can trigger specific animations to visualize the robot's operational cycle virtually overlaid on its physical twin.

- **Persistent Anchors:** Use reference images as durable AR anchors for placing and maintaining virtual content in the same spot.  
   > The robot marker acts as a stable anchor, ensuring the control UI and animation overlays stay aligned with the robot’s position even as users change their viewing angles or restart the AR session.

- **Interactive Games & Navigation:** Reference images can serve as checkpoints or directional cues in interactive AR experiences.  
   > In an industrial environment, printed markers placed in various stations can trigger contextual information or tutorials. For the robot arm, this could include maintenance instructions or safety guidelines appearing automatically when the robot’s marker is scanned.

- **Training and Skill Development:** Image tracking supports guided training modules where each marker represents a specific task or process step.  
   > Scanning a marker next to the equipment could launch AR instructions on proper equipment operation, troubleshooting, safety protocols, or common fault diagnosis.

- **Maintenance and Inspection:** Maintenance staff can scan reference images affixed to machines to reveal real-time data, part specifications, or last maintenance records.  
   > A marker on a CNC machine could show AR overlays indicating lubrication points, wear-prone components, or interactive checklists for preventive maintenance.

- **Digital Twins Integration:** Image tracking can help align virtual digital twins with their physical counterparts on the shop floor.  
   > A marker on an industrial 3D printer enables operators to overlay real-time process data like temperature, build progress, or upcoming tasks directly onto the printer in AR.

- **Marketing and Exhibits:** In showrooms or industrial exhibitions, image tracking can bring static displays to life with animations, data visualizations, or exploded views.  
   > Scanning a printed schematic of an engine can present an animated breakdown of each subsystem—pistons, valves, crankshaft—enhancing understanding for both technical and non-technical audiences.


---

# Reference Image Library

A `Reference Image Library` in Unity is a `ScriptableObject` that contains a collection of 2D images your AR app can recognize and track. You build and package this library with your Unity project to enable image tracking functionality. 


## What Is a Reference Image?

A **reference image** is a pre-defined 2D visual asset (such as a poster, sign, or schematic) that the AR system is trained to recognize. Each image can include an optional **physical width (in meters)**, improving tracking accuracy and scale fidelity. An effective reference image should be:

- **High-Resolution:** The image should contain ample pixel data for the AR system to extract and compare visual features. Low-resolution images may blur or pixelate, reducing detection reliability and precision.

- **Feature-Rich:**  Rich details like unique shapes, corners, and contrasting areas enable the computer vision algorithms to differentiate the image from others in the environment. Repetitive patterns (like grids or textures) can confuse tracking algorithms, leading to false detections or unstable tracking.

- **High-Contrast:** Sharp differences between light and dark regions enhance the system’s ability to identify and anchor feature points under varying lighting conditions. Low-contrast images may blend into the background or become difficult to detect in dim or bright environments.

- **Displayed on a Flat, Matte Surface:** Glossy or curved surfaces can cause reflections or distortions, which impede consistent tracking. A matte finish avoids glare from lighting, ensuring that the image is reliably visible to the device camera from different angles.

- **Real-World Dimensions:** Specifying the actual physical width of the image allows the AR system to scale virtual objects proportionally. Without accurate size data, the holographic overlays might appear misaligned, improperly scaled, or spatially disconnected from the physical reference.

> For basic AR applications, start by choosing images that already have distinct, recognizable features—such as company logos, equipment schematics, or instructional diagrams. Avoid images with repetitive textures or overly simplistic designs. Use Unity’s **AR Foundation feature point visualization tools** during development to verify that your selected image offers enough detectable features for reliable tracking.


## Types of Image Libraries

- **Static Library:** A static library is created in Unity's editor and packaged directly into the application build. The set of images cannot change at runtime. This is the most widely supported method across AR platforms (ARKit, ARCore, etc.) and is recommended when your set of target images is predefined.
  > For most AR applications where the target environment and markers are known ahead of time, a **static image library** is sufficient. Always ensure that the reference images are high-resolution and correctly sized (e.g., specifying a real-world width like `0.2m`) to maintain accurate detection and proper alignment of virtual content with the physical world.

- **Mutable Library:** This allows adding new images dynamically at runtime, but requires platform support (checked via `subsystem.subsystemDescriptor.supportsMutableLibrary`). In Unity, you can add new images using `ScheduleAddImageJob`. This is useful when the application needs to learn or update its targets post-deployment.
  > Mutable libraries are ideal for dynamic applications where users might introduce new materials, signage, or schematics that need to be recognized on the fly. This is common in scenarios like field maintenance, customizable product showcases, or educational tools that evolve with new reference materials—without requiring an app rebuild.


## Creating an Image Library

To enable image tracking in your AR project, start by creating a dedicated AR scene and then set up a `Reference Image Library` to store the images the app will recognize.

1. **Create the Image Tracking Scene:**
  - Open your `XFactoryAR` project.
  - In the Unity Editor, go to `File > New Scene > Basic (URP)`.
  - Right-click in the `Hierarchy` and select `XR > AR Session`.
  - Right-click in the `Hierarchy` and select `XR > XR Origin (AR)`.
  - Save the scene via `File > Save As`, naming it `Image Tracking.unity`.

    > If you already have an AR scene set up, duplicate it and rename it to **Image Tracking** for reuse.

2. **Create the Reference Image Library:**
  - In the top menu, navigate to `Assets > Create > XR > Reference Image Library`.
  - A new asset will appear in the `Project` window (default name: `ReferenceImageLibrary.asset`).
  - Rename the asset to something descriptive, such as `XFactoryImageLibrary`.
  - Optionally, move it to an appropriate folder to keep your `Assets` folder organized.

    ![01](/Figures/E4/01.jpg)

    > A specific name like `XFactoryImageLibrary` keeps your project organized, especially when handling multiple image tracking libraries.

3. **Import the Marker Image:**
  - Navigate to `Assets > Import New Asset…`.
  - Select the image file(s) you want to use as reference markers (e.g., PNG, JPG).
  - After importing, select each image and check its `Inspector`.
  - Confirm `Texture Type` is set to `Default`.
  - Ensure the image is high-resolution, has sharp contrast, and distinct features.

    ![02](/Figures/E4/02.jpg)

4. **Add Images to the Reference Image Library:**
  - Select the `XFactoryImageLibrary` asset in the `Project` window.
  - In the `Inspector`, click `Add Image` to create a new entry for each image.
  - Click `Select` under `Texture2D Asset` to assign the imported marker image.
  - Enter a `Name` for easy identification in scripts.
  - Enable `Specify Size` and input the `Physical Size (meters)` of the printed marker, e.g., `0.2` for 20 cm.
  - Leave `Keep Texture at Runtime` unchecked unless you need runtime access to the texture.

    ![03](/Figures/E4/03.jpg)

> Properly configuring these settings ensures reliable detection, accurate scaling of virtual content, and stable tracking performance, all of which are essential for a smooth and precise AR experience. With the scene and `Reference Image Library` configured, you are ready to integrate the `AR Tracked Image Manager` to recognize the marker and trigger the corresponding robot control interfaces in AR.


---

# AR Tracked Image Manager

In AR Foundation, the `ARTrackedImageManager` detects 2D images in the real world and spawns an `ARTrackedImage` GameObject when a match is found from your `Reference Image Library`. The `ARTrackedImage` acts as the anchor that keeps the spawned model aligned with the physical marker. 


## Core Concepts

- **Reference Image Library:** The `ARTrackedImageManager` uses this library to detect markers in the real world. As introduced earlier, a  `Reference Image Library` is a collection of images that the AR system can recognize. Each image in the library includes a unique name, the physical size of the image, and the image data itself. 

- **Image Detection and Recognition:** The `ARTrackedImageManager` continuously scans the camera feed for images that match those in the `Reference Image Library`. When it recognizes a marker, it spawns a corresponding `ARTrackedImage` GameObject in the scene.

- **ARTrackedImage GameObject:** This GameObject represents the detected image and serves as an anchor for placing virtual content. Any 3D models, UI panels, or holographic elements should be parented to the ARTrackedImage to ensure they remain aligned with the physical marker.

- **Tracked Image Prefab:** A predefined prefab assigned in the `ARTrackedImageManager`. When an image is detected, the manager automatically instantiates this prefab as a child of the `ARTrackedImage`, positioning it directly over the marker.

- **Pose Tracking:** The `ARTrackedImageManager` keeps the `ARTrackedImage`'s position and rotation updated in real time based on the camera's view of the marker. This ensures that virtual content stays properly aligned even as the user moves around.

- **Tracking States:** Each `ARTrackedImage` has a tracking state that can be `Tracking`, `Limited`, or `None`. This state indicates the quality of tracking and can be used to adjust content visibility or interaction logic.

- **Lifecycle Events:** The manager exposes the `trackablesChanged` event, which notifies your app when tracked images are added, updated, or removed. This is useful for triggering animations, spawning effects, or cleaning up content based on tracking activity.

> In the Heavy Duty Robot Arm example, these concepts work together to detect the robot's marker, spawn the robot model on top of it, and maintain the robot's alignment and interactivity as the user moves or repositions their device.

## Example: Robot Arm

Let's use XFactory's **Heavy Duty Robot Arm** prefab as an example to showcase image tracking with AR Foundation. When a specific marker is detected, the robot appears in AR, anchored to the marker, with a **screen-space UI** for triggering its pick-and-place animations: **Pick**, **Turn**, **Place**, and **Home**. The robot’s behavior reuses the same Animator setup and activation logic from the VR module, now adapted for AR interaction.

![04](/Figures/E4/04.jpg)

1. **Add and Configure `ARTrackedImageManager`**:
  - Select `XR Origin` and go to its `Inspector`.
  - Click `Add Component > AR Tracked Image Manager`. This component listens for images from your reference library and manages the associated tracked image objects.
  - `Serialized Library`: Assign the `XFactoryImageLibrary`. This tells the manager which images to recognize.
  - `Max Number Of Moving Images`: Optionally, define how many moving images to track simultaneously if hardware or use case requires a limit.
  - `Tracked Image Prefab`: Assign the `Heavy Duty Robot Arm` prefab from `Assets > XFactory > Prefabs > Robots`. This prefab appears anchored to the detected marker.

    ![05](/Figures/E4/05.jpg)

    > The `Trackables Changed` field in the `ARTrackedImageManager` is a UnityEvent that triggers whenever tracked images are added, updated, or removed in the AR session. This event is optional and can be skipped for this example.

2. **Create a UI Canvas for Robot Control**:
  - Open the `Heavy Duty Robot Arm` prefab in prefab mode.
  - Right-click on the prefab root in the `Hierarchy` and select `UI > Canvas`.
  - Set the `Render Mode` to `Screen Space - Overlay`.
  - Right-click the `Canvas` and select `UI > Button - TextMeshPro`. Rename to `HomeButton`, change text to `Home`. Adjust its size as needed.
  - Duplicate the button three times, renaming and relabeling them as `PickButton`, `TurnButton`, and `PlaceButton`.
  - Use `RectTransform` to position the buttons neatly in the canvas.

    ![06](/Figures/E4/06.jpg)

    > Since the `Heavy Duty Robot Arm` prefab is instantiated dynamically when the marker is detected, we will make sure the prefab itself contains the UI canvas with buttons, already set up to control the robot’s `Animator`. 

3. **Create a UI Controller Script:**
  - Create a new C# script called `RobotUIController`, attach it to the `Canvas` in the prefab. This code connects the UI to the robot's `Animator`:

    ```csharp
    using UnityEngine;

    public class RobotUIController : MonoBehaviour
    {
        public Animator robotAnimator;

        public void TriggerAnimation(string triggerName)
        {
            if (robotAnimator != null)
            {
                robotAnimator.SetTrigger(triggerName);
            }
        }
    }
    ```
  
4. **Configure the Script:**
  - In prefab mode, select the `Canvas` inside the `Heavy Duty Robot Arm` prefab.
  - In the `Robot UI Controller` component, drag the `Heavy Duty Robot Arm` GameObject into the `Robot Animator` field, which will automatically detect its `Animator` component.

    ![07](/Figures/E4/07.jpg)

5. **Connect Each Button to Trigger the Animator:**
  - Select the button in the prefab hierarchy.
  - In the `Button (Script)` component, find the `On Click ()` event list.
  - Click the `+` to add a new event.
  - Drag the `Canvas` (where `RobotUIController.cs` is attached) into the event field.
  - From the dropdown, select `RobotUIController → TriggerAnimation(string)`.
  - In the string parameter field, type the corresponding trigger: `Home` (for `Home Button`), `Pick` (for `Pick Button`), `Turn` (for `Turn Button`), and `Place` (for `Place Button`)
  - After wiring all buttons, click **Save** in the Prefab Mode toolbar to apply the changes.

    ![08](/Figures/E4/08.jpg)

6. **Test the Behavior:**
  - Build and deploy the AR app to your mobile device.
  - Point the device’s camera at the robot marker image.
  - The `Heavy Duty Robot Arm` prefab, along with the robot control panel UI, should appear anchored to the marker.
  - Tapping each button triggers the associated animation on the robot via its `Animator`:
     - **Home:** Home pose
     - **Pick:** Pick animation
     - **Turn:** Turn motion
     - **Place:** Place animation

> This creates a cohesive AR experience where marker-based tracking and interaction with virtual machinery are seamlessly integrated through a simple UI.


---

# ARTrackedImage

When a marker image is detected in AR Foundation, Unity spawns an `ARTrackedImage` GameObject. This component contains essential information about the detected image and its tracking status, allowing your AR content (like the `Heavy Duty Robot Arm`) to stay properly anchored and responsive to tracking conditions.


## Key Properties

- **`destroyOnRemoval`:** If `true`, the GameObject is destroyed when the image is no longer tracked. For the robot, keep this `false` to prevent the model from disappearing during brief tracking interruptions.

- **`referenceImage`:** Contains metadata (name, size, GUID) about the detected image from the **Reference Image Library**. Use this to identify which image was recognized, if your app tracks multiple markers.

- **`trackingState`:** Indicates tracking reliability: `None`, `Limited`, or `Tracking`. Useful for adapting visuals, animations, or interactions based on how well the image is being tracked.

- **`sessionRelativePose`:** Provides the current pose of the tracked image relative to the AR session space. This ensures precise alignment of the virtual content with the physical marker.

- **`updated` (event):** An event you can subscribe to in code to respond when the tracked image's pose or tracking state changes.


## Tracked Image Lifecycle

Tracked images managed by `ARTrackedImageManager` pass through three main lifecycle phases:

1. **Added:** The first time an image is detected. The robot arm prefab is instantiated and aligned to the marker. For example, display the robot in the **Home** pose when the image is first found.

2. **Updated:** Triggered each frame if the image’s pose or tracking quality changes. Always check the **trackingState** before updating content. For example, if tracking is `Limited`, reduce the robot’s opacity or pause its actions.

3. **Removed:** Called when the image is no longer tracked. If `destroyOnRemoval` is `true`, the `ARTrackedImage` GameObject is destroyed. For example, pause interactions with the robot and optionally fade out the model if the marker is lost.

> Managing these phases ensures that your AR content behaves predictably as tracking conditions fluctuate.


## Tracking States Reference

- **`None`:** The image is not currently being tracked, either because it was never detected or is fully out of view.

- **`Limited`:** Tracking is degraded due to occlusion, poor lighting, or excessive motion. Consider dimming visuals or pausing interactions when this state is active.

- **`Tracking`:** The image is actively tracked with reliable position and orientation. Full visuals and interactions should be enabled.

> Visual indicators (e.g., an on-screen status icon) can help users know whether tracking is stable.


## Best Practices

- Always parent your AR content under the `ARTrackedImage` GameObject. This way, when the tracked image moves, rotates, or scales in the real world, the virtual content automatically follows and stays correctly aligned.

- Avoid manually destroying tracked image GameObjects in your scripts. Let the ARTrackedImageManager handle them using the `destroyOnRemoval` setting, or simply disable the content when tracking is lost. This prevents accidental errors or unexpected behavior when the AR system removes images.

- Check the `trackingState` of the ARTrackedImage before performing any updates to your visuals, logic, or interactions. This helps you avoid applying changes when the marker is poorly tracked or not tracked at all, which could lead to jittery or incorrect behavior.

- Cache references to any spawned content or controllers associated with the tracked image. For example, keeping a reference to a robot control panel or instructional overlays allows you to quickly update, reset, or hide them without needing to search the scene or recreate them.

> In the **Heavy Duty Robot Arm** scenario, the robot model and the robot control panel UI are both parented under the tracked image. This ensures that the robot stays correctly positioned on the marker, and the UI remains accessible and aligned, even if the user moves around or tilts their device.


## Platform Support

Not all AR Foundation providers support image tracking. Here is the current support status:

| Provider Plugin                  | Image Tracking Supported |
| -------------------------------- | ------------------------ |
| Google ARCore XR Plug-in         | ✔️                       |
| Apple ARKit XR Plug-in           | ✔️                       |
| Apple visionOS XR Plug-in        | ✔️                       |
| Microsoft HoloLens via OpenXR    | ❌                       |
| Unity OpenXR: Meta Quest         | ❌                       |
| Unity OpenXR: Android XR         | ❌                       |
| XR Simulation                    | ✔️                       |


> Understanding the `ARTrackedImage` component and its lifecycle is key to creating stable, responsive AR experiences. Next, explore how to combine these tracking events with UI interactions to create fully interactive AR workflows.