---
layout: page
title: "A1. Introduction to XRE"
---

# The Reality-Virtuality Continuum

**Extended Reality (XR)** is a collective term for technologies that modify our perception of reality by integrating digital content with the physical world or by creating entirely virtual environments. XR spans the **Reality-Virtuality Continuum**, a spectrum ranging from the completely real to the completely virtual. This continuum includes three main categories: **Virtual Reality (VR)**, **Augmented Reality (AR)**, and **Mixed Reality (MR)**.

At its core, XR offers new **affordances**—it changes *how* users can perceive, interact with, and manipulate information and environments beyond the limitations of traditional 2D screens and interfaces. These technologies enable spatial interaction, immersion, and context-aware experiences that align more naturally with human perception and cognition.

## Virtual Reality (VR)

**Virtual Reality (VR)** provides a fully immersive digital environment, replacing the user’s sensory input from the real world with computer-generated stimuli. The primary affordance of VR is **total immersion**—users are visually, and often aurally, isolated from their physical surroundings and placed within a three-dimensional, interactive virtual space. VR enables:

- **Embodied Presence:** The sensation of "being inside" a virtual environment, where users can move, look around, and interact naturally within a simulated world.
- **Spatial Interaction:** Users engage with digital objects using body movements, gestures, or controllers, allowing intuitive manipulation of 3D elements.
- **Environmental Control:** Every aspect of the virtual world—visuals, sounds, scale, and physics—can be designed and modified without real-world constraints.
- **Perspective Shifting:** Users can experience environments or scenarios from different viewpoints or scales, such as viewing a model from human scale or shrinking down to inspect minute details.

> VR’s core strength lies in its ability to **immerse** users in scenarios that are detached from physical reality, offering a controlled, customizable environment for exploration and interaction.

## Augmented Reality (AR)

**Augmented Reality (AR)** introduces digital content into the user’s perception of the real world, overlaying information, graphics, or interactive elements onto the physical environment in real time. The key affordance of AR is **contextual enhancement**—it supplements reality rather than replacing it. AR enables:

- **Persistent Awareness of the Physical World:** Users remain grounded in their real environment while accessing additional layers of digital information.
- **Context-Aware Visualization:** Digital content is linked to real-world objects, locations, or tasks, providing relevant information precisely where and when it is needed.
- **Hands-Free, Heads-Up Interaction:** Through wearable devices, AR allows users to receive guidance or data without diverting attention to separate screens or manuals.
- **Spatial Anchoring:** Virtual elements can be fixed to specific points or surfaces in the real world, maintaining their position relative to the user’s movements.

> AR’s power is in **enhancing perception**—it enriches real-world experiences by embedding actionable, dynamic information into the user’s immediate surroundings.


## Mixed Reality (MR)

**Mixed Reality (MR)** refers to experiences where digital and physical elements coexist and interact in a shared space, with virtual objects behaving as if they are part of the real world. While MR is often viewed as an extension of AR, its distinct affordance lies in **interactive blending**—virtual objects are not only overlaid but are spatially aware and responsive to both the environment and the user.

MR enables:

- **Two-Way Interaction:** Digital elements can respond to physical surfaces, obstacles, and user inputs, creating a sense of integration between worlds.
- **Environmental Understanding:** Devices map the physical space in detail, allowing virtual content to exhibit realistic behaviors like collision, occlusion, or adherence to surfaces.
- **Natural User Interfaces (NUI):** Interaction is often driven by gaze, gestures, and voice, minimizing reliance on external controllers.

> While the term MR is becoming less emphasized—many AR systems now incorporate MR capabilities—the concept remains important in describing the **fusion** of real and virtual elements into cohesive, interactive experiences.

---
# XR’s Role Across Industries

XR has moved well beyond its entertainment roots; it is now a **horizontal technology layer** that reshapes every stage of the industrial value-chain—from ideation to after-sales service. Analysts project the total XR market will top **USD 2.8 trillion by 2033** as adoption accelerates across manufacturing, healthcare, energy, logistics, retail, and the public sector. Below is a concise, industry-spanning map of where (and why) XR is creating business value today.

**Training and Knowledge Transfer**
* **Risk-free VR simulations** build muscle-memory for hazardous, rare, or high-value procedures.  
* **On-the-job AR coaching** delivers step-by-step guidance at the moment of need, capturing and re-using expert knowledge as digital SOPs.  
* Industry studies show immersive training can cut training time by 40-60% while improving retention compared with classroom-only methods.

**Product & Process Innovation**
* **Immersive design reviews** let globally distributed teams co-create in full scale before anything is built, collapsing iteration cycles.  
* **Virtual prototyping and simulation** replace many physical mock-ups, cutting material waste and shortening time-to-market.  
* **Digital-twin integration** synchronizes real-world sensor data with live 3-D models, enabling continuous improvement throughout a product’s life.  
* **Industrial-metaverse** platforms are the next step, fusing XR, AI, IoT and simulation into persistent collaborative spaces.

**Operations and Maintenance**
* **Hands-free AR work instructions** guide technicians through complex tasks, reducing errors and downtime.  
* **Remote expert assistance** lets a specialist “see-what-I-see” and annotate the field worker’s view, slashing travel costs and mean-time-to-repair.  
* **Real-time performance overlays** (pressures, temperatures, KPIs) turn XR head-worn displays into live dashboards on the shop-floor or out in the field.

**Customer and Stakeholder Engagement**
* **Virtual showrooms and configurators** let buyers experience products at home or in pop-up venues, boosting confidence and conversion.  
* **In-context AR visualisation** (e.g., “see the machine in your plant” or “preview furniture in your room”) personalises the sales journey and reduces returns.  
* **Immersive marketing and demonstration** builds emotional connection and brand differentiation in crowded markets.

**Data-Driven Decision Support**
* XR dashboards turn large, complex data sets into intuitive spatial scenes that executives, engineers, and operators can explore together, leading to faster, consensus-driven decisions.  
* Coupled with AI, XR environments can surface anomalies, predict maintenance events, and recommend optimisations in real time.

**Safety, Compliance and Sustainability**
* **Scenario rehearsal** in VR prepares teams for emergencies without real-world exposure.  
* **AR safety overlays** highlight exclusion zones, lock-out/tag-out points, or live electrical hazards, reducing accidents.  
* By minimizing prototypes, travel, and rework, XR directly reduces **carbon footprints** and streamlines resource use.


## Sector Snapshot

| Sector | Primary XR Impact | Market Momentum* |
| ------ | ---------------- | ---------------- |
| **Manufacturing & Industrial** | Design-for-manufacture, line balancing, remote maintenance | *Industrial-metaverse* market forecast ≥ **$150 B by 2035** |
| **Healthcare & Life Sciences** | Surgical rehearsal, medical-image overlay, patient therapy | XR healthcare market CAGR **≈ 27 % (2025-33)** |
| **AEC (Architecture, Engineering, Construction)** | BIM visualisation on-site, clash detection, client walk-throughs | Rapid shift from 2-D drawings to interactive digital twins |
| **Energy & Utilities** | Remote inspection of offshore/remote assets, AR procedural guidance | Driven by safety, downtime costs, and ageing workforce |
| **Transportation & Logistics** | Warehouse picking optimisation, AR navigation, driver training | Fuelled by e-commerce growth and autonomous-vehicle R&D |
| **Consumer & Retail** | Try-before-you-buy, virtual stores, immersive brand activations | XR features becoming table-stakes for leading retailers |
| **Public Sector & Defence** | Mission rehearsal, situational awareness, infrastructure planning | Focus on readiness, resilience, and cost control |



## Benefits of XR in Industry

- **Enhanced Design Accuracy and Visualization:** Inspect 3D models at true scale, catch design issues early, and iterate virtually to reduce costly late-stage changes.

- **Improved Collaboration and Communication:** Remote teams meet as avatars around shared digital data, accelerating decisions and reducing travel.

- **Immersive Training and Skills Development:** Realistic simulations let workers practice complex or hazardous tasks safely, boosting knowledge retention and confidence.

- **Greater Efficiency and Productivity:** AR guidance speeds maintenance, assembly, and inspection; VR prototyping shortens product development cycles.

- **Cost Reduction:** Fewer physical prototypes, less downtime, and reduced travel translate into measurable savings and strong ROI.

- **Increased Customer Engagement and Sales:** Interactive try-before-you-buy experiences and virtual showrooms enhance customer confidence and brand loyalty.


## Emerging Trends in XR

- **Advancements in XR Hardware:** Lighter, untethered headsets with higher-resolution displays, wider fields of view, and integrated haptics are making extended use comfortable and mobile. Purpose-built devices—such as AR safety helmets or slim smart glasses—are poised to become standard tools on job sites.

- **AI Integration and Spatial Computing:** AI augments XR by enabling context-aware interactions, intelligent object recognition, voice and gesture control, and adaptive training scenarios. Spatial computing platforms combine AI with real-time 3D mapping to merge digital and physical data seamlessly.

- **5G Connectivity and Cloud XR:** High-bandwidth, low-latency 5G networks and edge computing let heavy graphics rendering occur in the cloud, streaming rich XR content to lightweight devices. This unlocks enterprise-scale, location-independent collaboration and large-area AR deployments.

- **Rise of the Industrial Metaverse:** Persistent digital twins of factories, construction sites, and supply chains allow engineers and operators to enter shared virtual spaces that mirror real-world assets. The industrial metaverse promises faster decision-making, predictive maintenance, and continuous optimization across distributed operations.

---

# XR Use Cases Across Industries

XR is rapidly evolving from experimental technology to an essential pillar of industrial innovation. By combining immersive visualization, real-time data, and intelligent interaction, XR reshapes design workflows, enhances training, and fosters seamless collaboration. As hardware improves, AI matures, and ubiquitous connectivity arrives, XR will increasingly blur the boundary between digital and physical realms—empowering the next generation of engineers to solve complex challenges more creatively and efficiently. Below are several specific examples of how XR is revolutionizing products, processes, and systems in various companies across the above use cases.


## ABB: Maintenance Operations

Industrial leader ABB leveraged [XR technologies, specifically AR, to revolutionize field maintenance operations](https://unity.com/blog/industry/best-practices-for-bringing-ar-applications-to-the-field). By developing the [ABB Ability™ Augmented Field Procedures](https://new.abb.com/news/detail/60801/abb-launches-abb-ability-augmented-field-procedures-to-drive-worker-safety-and-efficiency-across-the-energy-sector) application, ABB transformed traditional, paper-based workflows into a fully digital, interactive experience. This multiplatform solution enhances efficiency, safety, and communication for field and control room operators across industrial sites.

![ABB](/Figures/A1/ABB copy.jpg)

**Key Features**
- **Multiplatform AR Application:** Runs on mobile devices and Microsoft HoloLens for flexible field deployment.
- **Digitized Procedures:** Converts standard operating procedures (SOPs) into interactive, real-time AR guidance.
- **Real-Time Data Integration:** Syncs with control systems for immediate data capture and analysis.
- **Remote Communication:** Utilizes Microsoft Remote Assist for live collaboration between field and control room operators.
- **User-Centric Design:** 
  - Unobtrusive, ergonomic UI optimized for safety.
  - Intuitive interactions using gestures, gaze, and voice commands.
  - Automated workflows to minimize manual input.

**Benefits**
- **Enhanced Operator Efficiency:** Reduces dependency on lengthy, expensive training by guiding operators through AR.
- **Improved Safety:** Minimizes risks with clear, real-time instructions and unobstructed views.
- **Paperless Workflow:** Eliminates manual paperwork, reducing errors and administrative overhead.
- **Up-to-Date Procedures:** Ensures operators always follow the latest protocols.
- **Seamless Communication:** Bridges the gap between field and control room staff with live support and data sharing.
- **User Adoption:** Designed with AR novices in mind, incorporating feedback from real users to streamline usability.


## Air Force: Maintenance Training

[Taqtile](https://taqtile.com) is transforming training and maintenance operations through its XR solution, [Manifest](https://taqtile.com/manifest/), built on Unity. These systems empower frontline workers across industries—including defense, manufacturing, and transportation—to execute complex tasks with precision. A standout application is within the **U.S. Air Force**, where Manifest has revolutionized [aircraft maintenance training by reducing errors and bridging the skills gap](https://unity.com/case-study/taqtile-augmented-reality-training).

![Air Force](/Figures/A1/Airforce.jpg)


**Key Features**
- **AR-Based Work Instructions:** Step-by-step, hands-free guidance overlaying real-world equipment using devices like Microsoft HoloLens, Magic Leap One, and iPads.
- **Knowledge Capture:** Experts can easily record procedures without needing programming or 3D modeling skills.
- **Cross-Platform Support:** Compatible with AR/VR headsets, tablets, and web browsers.
- **Rapid Deployment:** Built on Unity for faster development and flexibility in XR solution delivery.
- **Error Reduction Tools:** Real-time AR assistance minimizes mistakes during task execution.

**Benefits**
- **Error-Free Maintenance:** In U.S. Air Force trials, minimally experienced Level-1 engineers using Manifest completed all aircraft maintenance tasks **without assistance or errors**, compared to multiple failures and errors using traditional methods.
- **Bridging the Skills Gap:**  Enables less-experienced personnel to perform specialized tasks confidently, reducing reliance on senior experts.
- **Increased Productivity:** Faster task completion and training cycles through immersive, on-the-job AR guidance.
- **Consistent Training Outcomes:** Standardizes procedures, ensuring repeatable, high-quality performance across teams.
- **Scalable Knowledge Transfer:** Captures expert knowledge once and deploys it anywhere, supporting remote or distributed teams.
- **Extended Use Cases:** Beyond training and maintenance, Manifest supports virtual product demonstrations for industries where physical showcases are impractical.


## Audi: Virtual Exhibit XR

Audi, renowned for its innovation and commitment to technology, partnered with [SXCES](https://www.sxces.com) and [Govar Studios](https://www.govar.de) to develop [Virtual Exhibit XR](https://unity.com/resources/audi)—an immersive experience showcasing the Audi Q6 e-tron using Apple Vision Pro. Originally designed for journalists and media professionals, this cutting-edge XR solution highlights how immersive technologies can effectively communicate complex technical products. 

**Key Features**
- **Immersive Real-Time 3D Visualization:** Leveraging Unity’s real-time engine and CAD data integration for precise, dynamic representations.
- **Seamless AR/VR Switching:** Flexible spatial experiences with the ability to toggle between AR and VR.
- **High-Resolution, Photorealistic Displays:** Optimized for Apple Vision Pro’s natural field of view and advanced display technology.
- **Didactic Optimization:** Designed to clearly communicate complex technical details in an intuitive, user-friendly manner.
- **Cross-Platform Development:** Integration of existing iOS AR solutions with visionOS using Unity PolySpatial for rapid iteration and deployment.

![Audi](/Figures/A1/Audi.jpg)


**Benefits**
- **Enhanced Technical Communication:** Enables users to explore intricate systems—like vehicle components or, in an Air Force context, aircraft systems or defense technology—in an easily digestible, interactive format.
- **Improved Training & Simulation:** The immersive environment can be adapted for mission planning, maintenance training, or operational briefings, reducing reliance on physical prototypes.
- **Flexible Deployment Across Departments:** Scalable solution usable across various units—from engineering and communications to training and public affairs.
- **Accelerated Development Cycles:** Unity’s real-time capabilities and PolySpatial integration streamline XR content creation for evolving hardware platforms.
- **Strategic Innovation Edge:** Demonstrates leadership in adopting spatial computing, setting a standard for leveraging XR in technical industries, including defense and aerospace.


## BMW: AR In-Vehicle Experience

BMW Group is pioneering the integration of [AR technologies to enhance the in-vehicle experience](https://unity.com/blog/industry/bmw-augmented-reality-glasses), focusing on safety, navigation, and passenger engagement. MW's research team has developed AR glasses prototypes that complement traditional head-up displays, offering drivers and passengers immersive, real-time information and entertainment.

![BMW](/Figures/A1/BMW.jpg)

**Key Features**
- **AR Glasses Integration:** Displays real-time driver assistance information such as navigation guidance, hazard alerts, and road signage directly in the driver's field of view.
- **Passenger Experiences:** Interactive features like in-car gaming and personal movie theater mode for passengers.
- **Advanced Tracking System:** Proprietary 6-degrees-of-freedom (6-DoF) tracking to stabilize AR content despite vehicle and head movements.
- **Expanded Field of View:** AR glasses extend beyond the limitations of traditional head-up displays, covering a larger portion of the real-world environment.
- **Adaptive Display Management:** Coordination between AR glasses, head-up displays, and vehicle interfaces to avoid overlapping visual information.

**Benefits**
- **Enhanced Driver Safety:** Reduces cognitive load by presenting critical information intuitively within the driver’s line of sight.
- **Immersive Navigation:** Precise, real-time navigation aids, including turn-by-turn arrows and hazard markers.
- **Passenger Engagement:** Transforms travel time with interactive entertainment options.
- **Future-Proof Compatibility:** Vision to standardize AR glasses integration across various manufacturers, similar to smartphone connectivity.
- **Efficient Development Workflow:** Unity's flexibility accelerates prototyping and visualization, making complex AR concepts accessible to stakeholders.
- **Innovation Leadership:** Continues BMW’s legacy of digital innovation, setting new standards for in-car XR experiences.


## Bosch Rexroth: Virtual Showroom

Bosch Rexroth, a global leader in drive and control technologies, leveraged XR technologies to revolutionize its sales approach by developing a [VR showroom](https://unity.com/resources/bosch-rexroth?isGated=false). This immersive solution allows customers to explore large, complex industrial hydraulic products in a fully interactive 3D environment, overcoming traditional challenges of physical demonstrations and boosting customer engagement, sales, and market exposure.

![Bosch](/Figures/A1/Bosch.jpg)


**Key Features**
- **Immersive Virtual Showroom:** Real-time 3D display of large industrial products using VR.
- **Virtual Product Teardowns:** Instant inspection of internal components within seconds.
- **Remote Accessibility:** Demonstrate products to anyone, anywhere, supported by Unity’s multiplayer functionality.
- **Integrated Documentation:** Access product information directly within the virtual environment.
- **Enhanced Trade Show Experience:** Showcase multiple large-scale products without physical transport.
- **Complement to Traditional Sales:** Combines interactive XR experiences with conventional sales materials.

**Benefits**
- **Faster Time to Market:** Reduced product demonstration setup from over a year to just two months.
- **Increased Customer Engagement:** Doubled interaction rates compared to in-person displays.
- **Cost Reduction:** Up to 5% savings on employee travel. Eliminates the need to ship heavy, costly demonstration units.
- **Expanded Market Reach:** Engage hundreds of prospects daily at events and trade shows.
- **In-Depth Product Understanding:** Enables customers to explore and understand complex products, fostering solution-oriented discussions.
- **Boosted Brand Visibility:** Attracts a broader audience, including younger engineers, enhancing brand recognition.
- **Remote Support:** Reduced need for on-site technical staff through virtual collaboration tools.


## CCH: Fighting Infant Mortality

Cincinnati Children’s Hospital has pioneered the use of XR through its **“Surgeons Without Borders”** initiative, [leveraging VR to combat infant mortality caused by congenital heart disease](https://www.youtube.com/watch?v=ItE_NS9wFx8). The project centers around the development of a **VR Surgical Simulation Suite** and a collaborative **Surgical Mediverse**, enabling global surgical planning and training. This initiative aims to bridge the healthcare gap in low-to-middle income countries by enhancing surgical expertise through immersive technology.

![CCH](/Figures/A1/CCH.jpg)

**Key Features**
- **VR Surgical Simulation Suite** for detailed 3D surgical planning.
- Creation of a **Surgical Mediverse** with multiuser collaboration capabilities and real-time multilingual translation.
- Advanced tools for placement of valves, medical devices, and complex baffles on 3D digital twins of patient hearts.
- Unity-based VR platform for cost-effective, online global collaboration.
- Technical support and funding through the Unity for Humanity grant.

**Benefits**
- **Improved Surgical Outcomes:** Enhances surgeon experience and precision in CHD procedures, especially in resource-limited settings.
- **Global Access & Collaboration:** Connects surgeons worldwide, allowing knowledge transfer and real-time cooperative planning.
- **Reduced Infant Mortality:** Targets preventable deaths from congenital heart defects in underserved regions.
- **Cost-Effective Training:** Provides an affordable, scalable solution for surgical education and planning without the need for physical presence.
- **Innovation in Healthcare:** Demonstrates how XR technologies can democratize access to advanced medical tools and expertise, transforming pediatric cardiac care globally.


## Gucci: Immersive Fashion

Gucci has embraced XR technologies alongside AI to revolutionize customer engagement and redefine the fashion retail experience. Gucci has been at the forefront of digital innovation, blending tradition with technology to offer immersive experiences that resonate with tech-savvy consumers, particularly Gen Z and Millennials.

![Gucci](/Figures/A1/Gucci.jpg)

**Key Features**
- **VR Showrooms and Fashion Shows:** Immersive digital environments allowing customers to explore collections and attend fashion shows virtually.
- **AR "Try-Before-You-Buy":** Smartphone-enabled virtual try-ons for eyewear, sneakers, and accessories, enhancing online shopping confidence.
- **XR Experiences:** Interactive digital spaces like [Gucci Garden](https://www.gucci.com/us/en/st/stories/article/gucci_garden?srsltid=AfmBOorfTzP2AVLpPiF3rf_dL3jfDmc2MxePXLhqWTTwKC9D-c8ld4Ou), [Gucci Cosmos Land](https://www.gucci.com/us/en/st/stories/article/cosmos-london-sandbox?srsltid=AfmBOorhO6ZGfa7FZfWzastIPvTMtdYXTvtEa1smpztBeArbG-ldXGu0), and collaborations in platforms such as Roblox and The Sandbox.
- **AI-Driven Personalization:** AI chatbots, personalized product recommendations, and data-driven design insights to enhance customer interaction and operational efficiency.
- **Integration with Spatial Computing:** Innovative immersive storytelling through 3D interactive experiences without direct commercial prompts, focusing on artistry and brand connection.

**Benefits**
- **Enhanced Customer Engagement:** Immersive VR/AR/XR experiences deepen emotional connections with the brand, offering unique and memorable interactions.
- **Global Accessibility:** Virtual platforms remove geographical barriers, allowing worldwide audiences to participate in exclusive events and experiences.
- **Personalized Shopping Journeys:** AI enhances customer satisfaction through tailored recommendations and efficient service, fostering loyalty.
- **Innovation Leadership:** Solidifies Gucci’s reputation as a digital pioneer in luxury fashion, consistently challenging industry norms.
- **Seamless Blend of Physical and Digital:** XR technologies bridge the gap between traditional retail and futuristic digital experiences, aligning with modern consumer expectations.
- **Sustainability and Efficiency:** AI-powered inventory management reduces waste, supporting Gucci’s sustainability goals.

## HD Hyundai Core: AR Guidance

HD Hyundai Infracore has integrated AR technology through its [AR Guidance](https://unity.com/blog/industry-first-ar-solution-for-construction-machinery) solution to revolutionize the maintenance and troubleshooting of construction machinery. AR Guidance overlays digital content onto real-world equipment, enabling dealers and service staff to intuitively diagnose and resolve complex machinery issues. This marks the first AR solution of its kind within the construction machinery industry.

![Hyundai](/Figures/A1/Hyundai.jpg)

**Key Features**
- **Real-Time 3D AR Visualization:** Interactive 3D models of construction equipment for enhanced understanding and faster troubleshooting.
- **Sensor Signal Monitoring:** Real-time status monitoring of machinery through AR-enhanced data visualization.
- **Troubleshooting Guides:** Fault code-based diagnostic assistance, including detailed component data (e.g., connector PINs, wiring diagrams).
- **Performance Test Guide:** Step-by-step guidance to verify equipment performance post-repair.
- **3D Model Viewer:** Non-AR module providing easy navigation of equipment models linked to system circuit diagrams for beginners.
- **Mobile Device Compatibility:** Optimized for smartphones and tablets, with future plans for AR glasses and VR integration.
- **Data Optimization:** Streamlined 3D content for stable performance in harsh construction environments.

**Benefits**
- **Reduced Service Time:** Faster fault identification and resolution, minimizing equipment downtime.
- **Enhanced Communication:** Simplifies data sharing between service staff and headquarters, improving remote support efficiency.
- **Improved Usability:** Intuitive, visual-based guidance reduces dependency on complex text-based manuals, especially for new staff.
- **Increased Client Satisfaction:** Positions HD Hyundai Infracore as a leader in digital technical support, enhancing their premium service image.
- **Scalable Digital Transformation:** Foundation for future XR developments, including AR glasses for hands-free maintenance and VR for training and inspections.
- **Global Reach:** Positive adoption among European service staff with plans to expand usage to over 80% of global service teams.


## HR Wallingford: Pilot Training 

HR Wallingford leverages XR technologies to revolutionize [vessel pilot training for ports and shipping operations](https://unity.com/resources/hr-wallingford). By creating highly realistic and customizable simulation environments, they improve pilot skills, enhance safety, and reduce training delivery times from months to weeks. This advanced simulator solution supports critical operations at ports like Harwich Haven Authority, handling 40% of the UK's container traffic.

![HR](/Figures/A1/HR.jpg)

**Key Features**
- **High-Fidelity Simulation:** Utilizes Unity's High Definition Render Pipeline (HDRP) for realistic wave, wind, and current modeling.
- **Real-Time Customization:** Integrates numerical wave and hydrodynamic models, allowing immediate updates for evolving vessel and port specifications.
- **Fully Simulated Bridge Environment:** Real-time replication of vessel behavior with accurate visual cues and environmental forces.
- **Multi-User Interaction:** Supports simultaneous operations involving vessels and multiple tugs.
- **Instant Feedback and Playback:** On-screen session reviews for debriefing and continuous learning.
- **Flexible Deployment:** Tailored training scenarios for new vessel classes, adverse weather, and operational limits.
- **Support from Industry Success:** Dedicated advisor guidance for efficient development and deployment.

**Benefits**
- **Enhanced Safety:** Risk-free training in complex and adverse conditions, reducing the chance of real-world incidents.
- **Reduced Training Time:** Development cycles shortened significantly without increasing team size.
- **Improved Pilot Proficiency:** Continuous professional development with realistic, repeatable scenarios.
- **Future-Proofing Operations:** Prepares pilots for new vessel classes and supports strategic port planning.
- **Inclusive Career Development:** Facilitates recruitment, assessment, and outreach initiatives through engaging simulation experiences.
- **Cost Efficiency:** Eliminates dependency on vessel availability and minimizes recruitment for technical development.
- **Operational Excellence:** Encourages best practices and supports independent assessments, such as towage requirements.


## KLM: VR Cockpit Trainer

KLM Royal Dutch Airlines, through its subsidiary KLM Cityhopper, leveraged XR technology to develop a [VR Cockpit Trainer](https://unity.com/resources/klm). This immersive training solution addresses the challenges of traditional aviation training by offering a flexible, scalable, and cost-effective way to train pilots on Embraer E175 and E190 aircraft procedures.

![KLM](/Figures/A1/KLM.jpg)

**Key Features**
- **Virtual Cockpit Simulation:** Interactive, realistic 3D cockpit environment for hands-on pilot training.
- **360-Degree Point-of-View Video:** Immersive cockpit jump seat experience to enhance situational awareness.
- **Virtual Walkaround:** Detailed 3D aircraft inspections to improve familiarity with aircraft structure.
- **Customizable Training Scenarios:** Easily adaptable procedures using a custom node system.
- **Hand-Tracking Integration:** Natural interaction within the VR environment.
- **Rapid Prototyping and Iteration** Real-time testing and adjustments via Unity Editor.
- **Multi-Platform Deployment:** Optimized for devices like Meta Quest 3.
- **Streamlined Asset Management:** Utilizing Unity Asset Manager for efficient asset handling.
- **Future Multiplayer Mode:** Planned feature for collaborative pilot training sessions.

**Benefits**
- **Reduced Development and Iteration Time:** Faster updates and scenario customization through Unity’s tools.
- **Enhanced Training Flexibility:** Pilots can train anytime, anywhere, accommodating personal schedules.
- **Cost Efficiency:** Eliminates reliance on expensive physical simulators and manual updates.
- **Improved Training Quality:** High-fidelity visuals, interactivity, and immersive scenarios boost learning outcomes.
- **Scalability:** Easily adapts to varying numbers of trainees and supports multiple platforms.
- **Increased Situational Awareness:** Realistic simulations prepare pilots for real-world scenarios.
- **Future-Proofing:** Ongoing enhancements like multiplayer ensure continuous value and innovation.


## Mazda: XR Cockpit

Mazda is adopting [XR to revolutionize automotive cockpit Human-Machine Interfaces (HMI)](https://unity.com/blog/industry/mazda-unity-partnership-automotive-cockpit-hmi) as part of its 2030 roadmap. By embedding Unity’s real-time 3D engine into future vehicles, Mazda aims to deliver a more intuitive, responsive, and human-centric in-vehicle experience. This strategic collaboration leverages XR technologies to enhance safety, personalization, and development efficiency across multiple automotive applications.

![Mazda](/Figures/A1/Mazda.jpg)

**Key Features**
- **Real-Time 3D Rendering:** Spatial representation of complex vehicle information for intuitive driver interaction.
- **Multiplatform Support:** Seamless adaptability across various hardware and software environments.
- **Efficient Development Workflows:** Integrated environment for designers, developers, and engineers to collaborate and iterate rapidly.
- **Personalized HMI:** Customizable interfaces tailored to individual driver preferences.
- **Cross-Departmental Integration:** Use of Unity for VR-based UX testing, prototyping, digital twins, design visualization, and car configurators.
- **Active Global Community:** Access to extensive resources, tools, and a broad talent pool of Unity developers.

**Benefits**
- **Enhanced Safety:** Reduced cognitive load through intuitive, real-time display of critical information.
- **Improved User Experience:** Human-centric design delivering responsive and visually appealing interfaces.
- **Faster Time-to-Market:** Streamlined development processes with reusable assets and game-industry-grade tools.
- **Future-Proof Technology:** Unity’s adaptability minimizes risks associated with technological evolution.
- **Collaborative Innovation:** Unified development environment fosters better communication and creativity across departments.
- **Increased Customer Value:** Creation of exciting, personalized mobility experiences aligned with Mazda’s vision of intuitive vehicle operation.

## VirtaMed: VR Surgical Training

[VirtaMed](https://www.virtamed.com/en) has been revolutionizing surgical education by creating advanced robotic surgical simulators in VR that offer realistic, immersive, and risk-free environments for surgeons and medical teams to practice and refine their skills. These VR-based simulations replicate real-world surgical robotics systems, enabling efficient, repeatable, and cost-effective training directly within hospitals, medical schools, and by device manufacturers.

![VirtaMed](/Figures/A1/VirtaMed.jpg)

**Key Features**
- **Realistic Surgical Simulations:** High-fidelity 3D environments using data-backed anatomical models and sensorized instruments.
- **Advanced Physics and Rendering:** Utilization of Unity’s Articulation Body physics, HDRP, and Shader Graph for lifelike visuals and precise interactions.
- **Haptic Feedback Integration:** Tactile responses through advanced haptic interfaces for hands-on practice.
- **Customizable and Modular Platforms:** Adaptable simulators deployable in hospitals, training centers, and conferences.
- **Multiplatform Deployment:** Seamless integration with various hardware setups, including portable "backpack" simulators and full console replicas.
- **Data-Driven Feedback:** Collection of detailed performance metrics with proficiency scoring for continuous learner improvement.

**Benefits**
- **Risk-Free Training Environment:** Surgeons can practice complex robotic procedures without involving live patients or occupying operating rooms.
- **Reduced Learning Curve:** Accelerates mastery of robotic surgical systems compared to traditional cadaver or lab-based training.
- **Cost Efficiency:** Minimizes reliance on expensive lab setups and reduces the need for physical resources.
- **Repeatable and Accessible:** Enables unlimited practice sessions, fostering skill retention and confidence.
- **Enhanced Surgical Outcomes:** Better-prepared surgical teams lead to improved patient safety and care quality.
- **Supports Continuous Innovation:** Flexible platform allows for rapid updates, new procedure modules, and adaptation to evolving surgical technologies.
- **Global Reach Through Partnerships:** Collaborations with surgical robot manufacturers and medical societies extend training access worldwide.



## Volvo: Collaboration via XR

Volvo Cars, a pioneer in automotive safety and innovation, leverages [XR technologies to transform its vehicle production lifecycle](https://unity.com/case-study/volvo). By integrating immersive virtual experiences across design, engineering, manufacturing, marketing, and sales, Volvo Cars enhances collaboration, accelerates innovation, and redefines both product development and customer engagement.

![Volvo](/Figures/A1/Volvo.jpg)

**Key Features**
- **End-to-End Virtual Toolchain:** Integration of Unity with CAD, AR, and VR platforms to unify workflows across departments.
- **Real-Time Collaboration**: Engineers, designers, and stakeholders visualize and interact with virtual vehicle models for faster iteration and validation.
- **Immersive Prototyping:** Reduced reliance on physical prototypes through virtual simulations and user testing.
- **In-Vehicle UX Simulation:** Real-time testing of ergonomics, interfaces, and driver interactions within virtual cabin environments.
- **Virtual Training Environments:** Safe, photorealistic VR training for production, maintenance, and end-user guidance.
- **Immersive Sales and Marketing Tools:** VR car configurators and AR experiences to engage buyers and collect behavioral insights.
- **Road Testing:** [Partnership with Varjo enables photorealistic XR overlays](https://varjo.com/testimonial/xr-test-drive-with-volvo/) during real-world driving tests, allowing rapid design studies, UX concept validation, and real-life scenario testing.

**Benefits**
- **Enhanced Cross-Department Collaboration:** A unified virtual environment bridges communication gaps between teams.
- **Accelerated Design and Development Cycles:** Quickly iterate design and UX concepts, accomplishing in a day what used to take weeks.
- **Cost Savings:** Avoids frequent tool and software changes, reducing reliance on physical prototypes and saving significant costs.
- **Improved Product Quality:** High-fidelity mixed reality from Varjo delivers unprecedented realism, increasing confidence in design evaluations.
- **Safe & Efficient Training:** VR-based training enhances workforce readiness without real-world risks.
- **Elevated Customer Experience:** Immersive configurators and virtual showrooms offer personalized, engaging buying journeys.
- **Advanced Testing Capabilities:** Test future vehicle features, safety systems, and UI in real cars on real roads using Varjo’s low-latency, high-resolution XR headsets.


## YVR Airport: XR Digital Twin

Vancouver International Airport (YVR) has implemented a groundbreaking [real-time XR digital twin solution](https://unity.com/case-study/vancouver-airport-authority) to transform airport operations by creating dynamic virtual replicas of physical assets and processes, enabling smarter decision-making, enhanced operational efficiency, and progress toward sustainability and reconciliation goals.

![YVR](/Figures/A1/YVR.jpg)

**Key Features**
- **Real-Time 3D Digital Twin:** Interactive 2D and 3D visualizations of airport operations, integrating historical and live data.
- **Situational Awareness Tool:** Consolidates real-time alerts and data anomalies, offering a bird’s-eye view of terminal activities for proactive response.
- **Carbon Emissions Tracking:** First-to-market model for real-time measurement and analysis of aircraft-related GHG emissions.
- **Scenario Simulation:** Ability to test hypothetical situations (e.g., security, weather) before real-world implementation.
- **Indigenous Training Program:** Partnership with Musqueam Indian Band to deliver 3D technology education, supporting future careers in XR and gaming industries.
- **Commercialization Potential:** Plans to adapt and offer the digital twin model to other airports globally.

**Benefits**
- **Operational Efficiency:** Improved resource allocation, faster response to issues, and minimized operational disruptions.
- **Cost Reduction:** Avoids costly errors through virtual testing and optimized maintenance planning.
- **Enhanced Safety:** Real-time monitoring increases situational awareness, ensuring passenger, staff, and cargo safety.
- **Sustainability Leadership:** Supports aggressive climate goals with precise tracking of carbon emissions.
- **Social Impact:** Advances Reconciliation by fostering Indigenous participation in tech innovation.
- **Future-Proofing:** Positions YVR at the forefront of digital transformation in aviation, with scalable solutions for the global market.


---

# XR Technology Stack

A **technology stack** is a way of describing the collection of technologies or layers that work together to create a functioning system. For XR, the stack includes everything from the hardware that captures inputs and displays outputs, to the software that processes data and renders immersive content. XR systems are often described in terms of layers, each responsible for different functions–from capturing the user’s input and environment, to processing that data, to producing the output that the user experiences. A simplified view of the XR stack from input to output looks like this:

1. **Input Layer (Sensors and Tracking):** Devices like cameras, motion sensors, gyroscopes, and depth sensors capture information about the user and the environment. For example, VR headsets track the orientation and position of your head, and AR systems use cameras (and sometimes LiDAR) to understand the environment. User input devices (like handheld controllers or hand-tracking cameras) also fall into this layer, registering actions like pointing, grabbing, or walking.

2. **Processing Layer (Computing and Algorithms):** This layer is where the XR device’s processor (or a connected computer/phone) takes the sensor data and makes sense of it. The system calculates the user’s movements, updates the positions of virtual objects, and runs the physics and logic of the virtual world. For AR, this layer might involve plane detection (finding surfaces in the real world), and for VR it includes things like collision detection in the virtual environment. The processing layer needs to work **in real time**, meaning it performs all these calculations very quickly (usually many times per second) so that the virtual content responds immediately to the user’s actions.

3. **Rendering Layer (Graphics and Audio Generation):** Once the system has updated the virtual world state, it then renders the scene – i.e. it generates the images (and sounds) that need to be presented to the user. A **rendering engine** or game engine (like Unity or Unreal, which we will discuss later) takes care of drawing the 3D graphics. It considers the user’s current viewpoint and produces the correct images for each eye in a VR headset, or the right overlay graphics for an AR view. Similarly, spatial audio engines generate sound that comes from the proper direction and distance in the virtual scene.

4. **Output Layer (Display and Feedback):** This is what the user ultimately experiences. In VR, the output is typically shown on the displays inside a headset (one screen per eye) along with audio through headphones. In AR, the output might be graphics superimposed on a smartphone screen or on see-through glasses. Output can also include other feedback devices. For example, **haptic** feedback (vibrations or other tactile signals) to make interactions feel more real.

## Real-Time Interaction

All these layers work together as part of the XR stack to create a **seamless experience**. For example, when you move your head or walk around, the sensors (input layer) feed data to the system, the processing layer updates the virtual world, the rendering engine generates new visuals, and the displays (output layer) show you an updated scene – all in a fraction of a second. This emphasis on **real-time interaction** is crucial: XR systems strive to minimize any **latency**, which is the delay between a user’s action and the system’s response. Even small delays (just a few milliseconds) can make interactions feel unnatural or laggy. 

- In VR, high latency can lead to **motion sickness**, because your brain senses a mismatch between what your body is doing and what your eyes are seeing. 
- In AR, lag can break the illusion that virtual objects are really part of your environment—for example, if a virtual label “sticks” to a moving object but trails behind it.

To ensure smooth, convincing experiences, XR systems aim to achieve **low-latency performance**, often targeting frame updates every 10 to 20 milliseconds (which corresponds to 60 to 90+ frames per second). Achieving this requires powerful processors, efficient algorithms, and optimized graphics pipelines. The system needs to:

- **Continuously capture sensor data** from motion trackers, IMUs, or cameras
- **Instantly calculate new positions and orientations** for the user and objects
- **Render new visuals from the user’s updated perspective** at high resolution
- **Synchronize audio and haptic feedback** to match visual updates

Real-time interaction also extends beyond basic motion. It includes detecting user gestures, voice commands, or controller inputs and immediately applying those to the virtual world—such as grabbing a virtual object or opening a menu. The ultimate goal is to make the user feel like they are truly present in and able to affect the XR environment without thinking about the technology.


## Spatial Computing

Another key concept in XR is **spatial computing**—the idea that the computer not only processes information but does so in the context of **3D physical space**. It means that the system understands where things are in the real world, where the user is in relation to them, and how digital elements can exist and behave within that shared space. In XR, spatial computing powers the system’s ability to:

- **Track Position and Orientation:** Devices constantly calculate their own location in space (called “pose tracking”) so that virtual objects can be anchored in place relative to the real world. This allows, for instance, a virtual chair to stay “on the floor” as you walk around it.
- **Map the Environment:** Using cameras, LiDAR, or other sensors, XR systems create a digital model of the real world. This might include flat surfaces (like tables and walls), environmental geometry, or even recognized objects.
- **Enable Spatially-Aware Interactions:** The system knows, for example, that a virtual ball should bounce off the real floor or that a holographic window should snap onto a real wall. It can also prevent virtual objects from passing through real ones, which preserves the illusion of solidity and realism.
- **Manage Occlusion:** Spatial computing helps determine whether a virtual object should appear in front of or behind real-world objects from the user’s point of view. If a holographic cat walks behind your sofa and becomes partially hidden, that’s spatial computing at work.

Beyond just understanding space, spatial computing allows systems to **reason about space**—making decisions based on spatial data. For example, in a collaborative AR scenario, spatial computing might ensure that all users in a room see a holographic model in the same place, properly aligned to the physical space. Spatial computing is what enables users to treat virtual content as if it were real—reaching out to touch it, walking around it, or integrating it naturally into their workflows. As XR technology advances, spatial computing is becoming more sophisticated, enabling persistent experiences (where virtual content stays in place even after the system restarts), shared multi-user environments, and more complex real-world interactions.

---
# XR Hardware

The XR technology stack relies on specialized hardware to capture inputs and present outputs. Here we introduce the major hardware components in XR systems, from the headsets you wear to the sensors and controllers you use to interact with virtual content.

## Headsets

Headsets, also referred to as Head-Mounted Displays (HMDs), are devices worn on the head that place images directly in front of your eyes. They are the primary hardware for most VR and MR systems:

- **VR Headsets:** These headsets completely cover your field of view with screens, fully immersing you in a virtual environment. Examples include the [Meta Quest](https://www.meta.com/quest/quest-3/?srsltid=AfmBOoo3H_6tVURcFHcRhRTUWKNfSb5d87nYRtyd28m-_fn9knrZF0_H) (formerly Oculus Quest), [HTC Vive](https://www.vive.com/us/), and [Varjo](https://varjo.com) headsets. VR headsets usually have built-in displays (one for each eye) and lenses that focus and enlarge the image. They also include sensors (like accelerometers, gyroscopes, and often external or **inside-out** cameras) to track head movement so that the view in the virtual world updates as you look around. High-end VR headsets often come with handheld controllers for interaction, and some are tethered to a PC while others (like the Quest) are standalone (all processing is done in the headset itself).

- **AR Headsets:** These are headsets with transparent or see-through displays, allowing you to see the real world while layering digital images on top of it. Examples include the [Microsoft HoloLens](https://learn.microsoft.com/en-us/hololens/) and [Magic Leap](https://www.magicleap.com). Instead of opaque screens, AR headsets use transparent optics (such as waveguides) to project digital light into your eyes, so virtual objects appear as holograms superimposed on your environment. These devices come with multiple cameras and sensors to track your head and hand movements and to map the real environment (so virtual objects can be placed correctly on surfaces and relative to real objects). They often have on-board processors, since they are self-contained computers. They may also use gesture recognition or voice commands for input, in addition to optional controllers.

## Mobile AR

You don’t always need a dedicated headset to experience XR. **Mobile devices** like smartphones and tablets can act as XR platforms; well, AR to be exact. In mobile AR, the phone or tablet’s screen becomes a “window” through which you see the augmented world. The device’s camera captures the real world and displays it on screen, while the software overlays digital content on that live camera view. A great example is using your phone’s camera to see how a piece of furniture would look in your room via an AR app–the furniture is rendered on the screen as if it’s in your room when you view it through the phone.

Modern smartphones are quite powerful and come packed with sensors ideal for AR. They typically have high-resolution cameras, IMUs (inertial measurement units) for motion tracking, and even **LiDAR** scanners (Light Detection and Ranging) in some devices (like recent iPhone and iPad Pro models) for depth sensing. Platforms like [Apple’s ARKit](https://developer.apple.com/augmented-reality/arkit/) and [Google’s ARCore](https://developers.google.com/ar) leverage these sensors to allow developers to create sophisticated AR experiences on mobile devices. Mobile XR is very accessible, since many people already have smartphones, and it doesn’t require any special hardware beyond the device. However, it typically offers a less immersive experience compared to using a headset, because you’re still looking at a screen in your hand rather than seeing the digital content in your full field of view.

## Projection Systems

XR can also be experienced through **projection-based systems**, which use projectors to blend digital content with physical environments. These setups are often used for specialized applications, collaborative experiences, or large-scale immersive environments. Projection systems are typically fixed to specific locations and require controlled environments to function effectively, especially in terms of lighting and surface preparation.

- **CAVE Systems (Cave Automatic Virtual Environment):** A CAVE is a room-sized VR environment where images are projected onto multiple surfaces—walls, floors, and sometimes ceilings—to create an immersive virtual space. Users typically wear 3D glasses to perceive depth, allowing them to experience a shared virtual environment without needing headsets. CAVEs are particularly useful for group visualization, such as architectural walkthroughs, scientific simulations, or industrial design reviews. While not as common as head-mounted displays due to their size and cost, CAVEs offer a unique way to engage multiple users in VR experiences simultaneously.

- **Projection-Based AR (Projection Mapping):** In projector-based AR, digital visuals are projected directly onto real-world surfaces, effectively turning any physical object or space into an interactive display. This technique, known as **projection mapping**, allows virtual content to conform to the shapes and textures of physical environments. For example, a projection system can overlay dynamic information onto a factory machine to guide maintenance tasks, or transform a blank tabletop into an interactive game board. Unlike AR through glasses or mobile devices, projection-based AR doesn’t require users to wear or hold any hardware—making it ideal for public installations, museums, exhibitions, and collaborative workspaces. Advanced systems may include sensors to detect user interactions, such as hand gestures or object movement, enabling real-time interaction with the projected content.


## Sensors and Peripherals

To interact with XR effectively, various sensors and peripheral devices are used in conjunction with the main display. These capture user input and provide additional feedback:

- **Cameras and Depth Sensors:** Cameras are fundamental for AR devices to "see" the world. For example, AR headsets and mobile AR use cameras to detect surfaces and track movement. Depth sensors like **LiDAR** or time-of-flight cameras measure distances to objects in the environment, creating a depth map. This helps an XR system understand the 3D layout of the real world – crucial for placing virtual objects believably and for safety (knowing where walls and obstacles are). The Microsoft HoloLens, for instance, uses a depth sensor to map rooms and track objects in real time.

- **Motion Trackers (IMUs):** Most XR hardware includes an IMU–a combination of accelerometers and gyroscopes (and sometimes magnetometers)–to track the orientation and acceleration of the device or controller. These sensors update rapidly (hundreds of times per second) to detect movements. High-end VR systems (like [Varjo XR-4](https://varjo.com/products/xr-4/)) also use external tracking cameras or base stations that track infrared markers on the headset and controllers for very precise position tracking in a room-scale space.

- **Handheld Controllers:** VR headsets often come with **motion controllers**–handheld devices with buttons, triggers, and tracking sensors. They let you grab objects, point, and interact with the virtual world. For example, [Meta Quest controllers](https://www.meta.com/quest/quest-3/?srsltid=AfmBOopea6btc9Q6bsmV-TAxqdLPo_8JImOt9jiiu7Y4VTI9X-faHFRs) and [HTC Vive wands](https://www.vive.com/us/accessory/controller/) are tracked in 3D space so the system knows where your hands are. These controllers usually provide basic haptic feedback (a rumble or vibration) to make interactions like hitting an object or firing a laser feel more tangible.

- **Hand Tracking:** In addition to or as an alternative to handheld controllers, some XR systems enable **hand tracking**, where your bare hands act as the input. Cameras on a headset (or external devices like the Leap Motion sensor) can analyze the movement of your hands and fingers, allowing you to touch and manipulate virtual objects directly with your hands. This increases immersion since you don't need any gadgets – your natural gestures become the controller. Many modern VR and AR headsets (like Quest 3, HoloLens 2, Magic Leap 2) have built-in hand tracking capabilities through their cameras.

- **Haptic Feedback:** These devices provide tactile sensations to match what’s happening in the XR experience. The simplest haptic feedback comes from vibration motors in controllers (providing a buzz when you collide with something). More advanced haptic devices include gloves that give force feedback on your fingers (to simulate the feeling of touching a solid object) or vests that let you feel impacts on your torso. While still an emerging area, haptics aim to engage the sense of touch, making XR experiences more immersive by adding a layer of physical feedback.


---
# XR Software

On the software side, the XR stack includes the engines, frameworks, and systems that create and run XR content. These software components handle everything from generating 3D graphics to interfacing with the hardware devices.

## Rendering Engines

A **rendering engine** is the software that draws the virtual world and handles the logic of the experience. In XR development, this is typically a game engine–a software framework originally designed for video games, which is also well-suited for interactive simulations in XR. The two most popular engines for XR are [Unity](https://unity.com) and [Unreal Engine](https://www.unrealengine.com/en-US). Both Unity and Unreal support all major XR hardware through plugins or integration with XR SDKs.

- **Unity:** Unity is a widely used cross-platform game engine known for its accessibility and a rich ecosystem for developers. In XR, Unity plays a critical role by providing the tools to build virtual environments and interactions. With Unity, developers can design scenes (arranging 3D models, lights, and cameras), apply physics (so objects have gravity or collide realistically), and write scripts that define how the world behaves. Unity’s real-time rendering capabilities mean it can generate the graphics for VR/AR at the high frame rates needed for smooth experiences. For example, Unity has built-in support for VR headsets and AR through packages like [AR Foundation](https://docs.unity3d.com/Packages/com.unity.xr.arfoundation@6.1/manual/index.html) and the [XR Interaction Toolkit](https://docs.unity3d.com/Packages/com.unity.xr.interaction.toolkit@3.1/manual/index.html), which help manage display, tracking, and user input for various devices. Its engine takes care of rendering separate images for each eye in VR (for stereo vision), handling details like perspective and lens distortion, and it integrates with device SDKs so that the developer’s code can easily use head tracking data or trigger haptic feedback. Because of its user-friendly interface and strong community, Unity is often the first choice for new XR developers.

- **Unreal Engine:** Unreal Engine, developed by Epic Games, is another powerful real-time 3D engine commonly used in XR, especially when high-fidelity graphics are required. Unreal is known for its advanced rendering quality (it’s often used in cutting-edge VR experiences with very realistic visuals). Like Unity, it provides a framework for building 3D worlds, simulating physics, and handling input. Unreal Engine offers a visual scripting system called [Blueprints](https://www.unrealengine.com/fr/blog/introduction-to-blueprints), which can be helpful for designers to create interactions without writing code. While Unity currently has a larger share of the XR market (due to its ease of use and early focus on mobile/standalone VR), Unreal is frequently used for high-end simulators, complex VR training systems, and AR projects that demand top-notch graphics. 

## XR SDKs and APIs

To build XR applications, developers rely on **Software Development Kits (SDKs)** and **Application Programming Interfaces (APIs)** provided by platform creators or standard bodies. These SDKs/APIs make it easier to access device features and sensors, so developers don't have to start from scratch for each hardware type. Some key SDKs/APIs include:

- **[ARKit](https://developer.apple.com/augmented-reality/arkit/):** ARKit is Apple’s AR development framework for iOS devices. It provides tools to build augmented reality apps for iPhones and iPads. ARKit handles tasks like motion tracking (using the phone’s IMU and camera to track the device’s movement in space), environment understanding (detecting surfaces like floors and tables, recognizing images or objects), and light estimation (so virtual objects can match the lighting of the real environment). Developers use ARKit through Apple’s APIs (in Swift or Objective-C), and the framework takes care of the heavy lifting of tracking and mapping the world. Many popular AR apps on the iPhone–from measuring tools to games–use ARKit under the hood.

- **[ARCore](https://developers.google.com/ar):** ARCore is Google’s platform for building AR applications on Android. Similar to ARKit, ARCore lets developers create AR experiences on common smartphones and tablets. It performs motion tracking (also called **visual-inertial odometry**, which combines camera input with inertial sensor data to track the device’s position as it moves), detects flat surfaces in the environment (for example, finding a table or floor where virtual objects can be placed), and estimates lighting conditions. Developers can use ARCore via the Android SDK or in engines like Unity (Google provides plugins for Unity and Unreal). ARCore and ARKit have many similar capabilities – in fact, both are moving toward the open standard OpenXR to make AR apps more portable across different devices.

- **[OpenXR](https://www.khronos.org/openxr/):** OpenXR is an open, royalty-free standard API developed by the Khronos Group that aims to unify the XR ecosystem. Instead of each VR/AR device having its own proprietary SDK, OpenXR provides a common set of functions for interacting with XR hardware. This means an application built with OpenXR can run on different brands of headsets or glasses without needing separate code for each. For example, an OpenXR app can run on a [Windows Mixed Reality headset](https://learn.microsoft.com/en-us/windows/mixed-reality/enthusiast-guide/), a [Meta Quest](https://www.meta.com/quest/quest-3/?srsltid=AfmBOooxwwvfX7OCCWgIbLhDTiJ7zT3PbFpB9u701n2-NCJwUhk2DbJW), or a [Valve Index](https://store.steampowered.com/valveindex), as long as each system supports the OpenXR runtime. Unity and Unreal Engine both support developing XR applications using OpenXR, so developers can write code once and deploy it to multiple platforms with minimal changes. This standardization greatly reduces fragmentation and makes life easier for XR developers and companies.

## OS and XR Runtime

The last piece of the XR stack involves the operating systems and runtimes that host XR applications. An **operating system (OS)** is the core software on a device that manages hardware and runs other programs. XR devices either use standard OSes with added XR support, or specialized OSes tuned for immersive technology.

- **XR on Standard Platforms:** Many XR experiences run on common operating systems. For instance, smartphone AR apps run on **iOS** or **Android**–these mobile OS platforms include support libraries (like [ARKit](https://developer.apple.com/augmented-reality/arkit/) on iOS or [ARCore](https://developers.google.com/ar) on Android) to access the device’s camera, motion sensors, and graphics processing for AR. On PCs, VR applications often run on **Windows** (or sometimes Linux). The OS provides basic drivers and graphics support (through technologies like DirectX or OpenGL), while XR-specific functions are typically handled by a separate runtime or API (such as OpenXR or a vendor-specific SDK like the Oculus VR SDK).

- **Specialized XR Operating Systems:** Some devices have custom-tailored OS environments for XR. The Microsoft HoloLens headset, for example, runs on a version of Windows 10 (known as Windows Holographic) that is designed for mixed reality. This OS manages the device’s multiple cameras, depth sensor, and holographic display, and it provides core services like spatial mapping (so the headset can understand walls, floors, and objects in your room). Similarly, Meta’s Quest VR headsets run on a customized Android-based OS optimized for virtual reality. This OS handles the headset’s inside-out tracking (using built-in cameras) and manages the launching of VR apps. These specialized OSes often include a **shell** or home environment in XR – for example, the Quest presents a virtual home space as its menu, and HoloLens has a holographic start menu floating in your real environment.

- **XR Runtimes:** Whether built into the OS or installed as software, an XR runtime is the layer that interfaces between XR applications and the hardware. For example, when you launch a VR game on a Windows PC, the SteamVR runtime (or Windows Mixed Reality runtime, depending on your headset) takes over to handle communication with the VR headset and controllers. It translates the game’s generic API calls into device-specific instructions. Likewise, on a phone, ARKit or ARCore acts as a runtime service managing the device's sensors and letting apps request things like “tell me where the surfaces are in the camera view” without dealing with low-level sensor data. The runtime ensures that developers don’t have to reinvent tracking and rendering for each device – it abstracts those details. In essence, the XR runtime and the OS work together to support real-time interaction by providing applications the needed access to hardware (displays, cameras, sensors, etc.) in a stable and standardized way.


---

# Putting It All Together

We just explored the foundational concepts behind XR and how its technology stack comes together to deliver immersive experiences. XR relies on a seamless integration of hardware and software components, each playing a critical role in enabling real-time interaction between users and digital environments. At the core of XR systems is a **layered architecture**, where:

1. **Input devices and sensors** capture data about the user’s movements, gestures, and the surrounding environment.
2. The **processing layer** computes spatial information, interaction logic, and updates the virtual scene.
3. The **rendering engine** generates high-quality graphics, audio, and other sensory outputs.
4. Finally, the **output layer** delivers the experience through displays, speakers, and haptic feedback devices.

The **hardware** side of XR includes a wide range of devices—from fully immersive **VR headsets** and transparent **AR headsets**, to accessible **mobile devices** like smartphones and tablets, and even **projection-based systems**. Supporting these are critical peripherals such as cameras, depth sensors, motion controllers, and haptic devices that enhance interaction and realism.

On the **software** side, powerful **game engines** like **Unity** and **Unreal Engine** serve as the backbone for creating XR content. These engines manage 3D environments, physics, rendering, and user interactions. Developers leverage **SDKs** (like ARKit, ARCore, and OpenXR) to interface with diverse hardware, ensuring compatibility and simplifying access to advanced XR features. Underpinning everything are **operating systems** and **runtime environments**, which coordinate hardware resources and maintain smooth, responsive experiences.

A key theme throughout the XR stack is **real-time spatial computing** — the ability of systems to understand and react to 3D space instantly, making virtual content feel natural and integrated with the user’s movements and environment. By understanding how these hardware and software components interact within the XR technology stack, you now have a clear conceptual framework for how XR experiences are built and delivered. This foundation will support your journey into more advanced topics, including XR development, interaction design, and system optimization in future modules.