---
layout: page
title: "D5. Activating Objects in VR"
---

> # Pre-Class Prep  
> **Estimated Prep Time:** 45–60 minutes  
> - **[Activation Events](https://github.gatech.edu/mmoghaddam3/XRE/wiki/D5.-Activating-Objects-in-VR#activation-events):** Review how Unity’s `XRBaseInteractable` and UI `Button` components expose UnityEvents like `OnSelectEntered()` and `OnClick()`. Learn how these events act as activation triggers for downstream logic (e.g., triggering animations, sounds, and state changes). Optionally explore example code that logs messages, plays audio, or changes light color on activation.
> - **[Event-Driven Design](https://github.gatech.edu/mmoghaddam3/XRE/wiki/D5.-Activating-Objects-in-VR#event-driven-design):** Understand the fundamentals of event-driven programming in Unity and XR: events, listeners, handlers, and modularity. Walk through the virtual factory’s robot controller example, where each numpad button triggers different robot animations. Practice using UnityEvents on buttons to call methods like `TriggerAnimation("Pick")`, and observe how the system responds using state-based logic.
> - **[Feedback Mechanisms](https://github.gatech.edu/mmoghaddam3/XRE/wiki/D5.-Activating-Objects-in-VR#feedback-mechanisms):** Learn how visual (lights, color changes), audio (sound effects), and tactile (haptic feedback) cues create clear system responses. Experiment with the `RobotFeedbackManager` system to play a sound or flash an object.
> - **[Contextual Interactions](https://github.gatech.edu/mmoghaddam3/XRE/wiki/D5.-Activating-Objects-in-VR#contextual-interactions):** Explore how activation logic can be gated by system state. Study how a global state manager disables robot interaction when E-stop is active, and provides visual/audio alerts. Try building or testing a conditional flow where button presses are ignored unless the system is in a ready state—use status lights and buzzers for clear feedback.

---

# Activation Events

Activation events in VR refer to specific **triggers** or **signals** that are generated when a user interacts with a virtual object or environment in a meaningful way. These events typically indicate **a change in state or the initiation of an action based on user input**. An activation event might occur when a user presses a button on a VR controller while grasping a virtual object, causing the object to perform an action such as opening, lighting up, transforming, or being thrown.  They are used to:

- **Provide Feedback:** Trigger sounds, change indicator lights, or update display panels.
- **Initiate Processes:** Start machinery, toggle system states, or signal a change in a simulation.
- **Decouple Actions:** Separate the user input from the actions performed, enabling modular programming.

> In the welding station of XFactory, pressing a virtual button on the `Controller` box with a touch-enabled numpad can trigger activation events for the `Heavy Payload Robot Arm`. For instance, pressing `1` could activate the `Pick` animation by setting the corresponding trigger in the `Animator Controller`. Upon activation, the system plays a confirmation beep, the `Status Light` indicator on the `Controller` box changes from **green** to **red**, and the `Heavy Payload Robot Arm` GameObject begins moving to pick up the car body from the trolly. This example illustrates how user interaction in VR can drive animation transitions, audio feedback, and visual state indicators in a synchronized manner.

![01](/Figures/D5/01.jpg)


## Principles

- **Event-Driven Design:** Activation events follow an event-driven design. When an event is detected (such as a button press), one or more functions execute. This modularity simplifies management and adapts well to complex engineering workflows.
  > At the welding station of XFactory, a user presses a number on the robot control box's touch numpad to initiate the heavy payload robot arm. The button press event triggers two modular actions: one that transitions the robot’s animation state (e.g., to "Pick"), and another that logs the action for monitoring.

- **Feedback Mechanisms:** Effective VR systems use both auditory (sounds) and visual (lighting, material changes) feedback. These cues are vital in engineering environments for communicating machine states and ensuring operator awareness.
  > When the heavy payload robot arm at the welding station begins moving, the status light on the controller box switches from green to red, and a mechanical hum plays to signal motion. When the robot returns to idle, the light turns green again and a soft chime sounds.

- **Contextual Interactions:**  Activation events should reflect the current system state, enabling or restricting interactions based on context (e.g., safety conditions, system readiness). This ensures realistic and safe behavior, mirroring industrial safety protocols.
  > If the emergency stop button at the welding station is engaged, pressing any number on the numpad will not activate the robot. Instead, a red flashing light is shown and an error buzzer sounds, indicating that activation is disabled until the emergency stop is cleared.

---

# Event-Driven Design

Event-driven design is a software architecture pattern in which the flow of the program is determined by events—user actions (such as button presses or controller inputs), sensor outputs, or messages from other systems. Rather than following a linear sequence, event-driven systems respond dynamically to triggers, making them ideal for XR environments where user interaction is unpredictable and interactive.

## Core Concepts

- **Events:** Discrete signals that indicate something has happened (e.g., button clicked).
- **Listeners/Subscribers:** Functions or methods that wait for and respond to these events.
- **Event Handlers:** The code that executes in response to an event.
- **Modularity:** Systems are loosely coupled—components can trigger or handle events without needing to know each other’s internal details.
- **Decoupling:** Changes in one part of the system (e.g., the numpad layout) do not require changes elsewhere (e.g., the robot logic).

> In XR development, event-driven design promotes clarity, flexibility, and scalability. For example, different control schemes or input devices can map to the same robot logic without rewriting the core system.


## Example: Controlling a Robot

At the welding station of XFactory, the user controls a heavy-duty robot arm using a numpad with four configurable buttons. Each button maps to a different action (e.g., `Pick`, `Turn`, `Place`, `Home`). These actions trigger the corresponding animation state and update the system's status indicator. Our goal is therefore to (1) map each numpad button (`1`–`4` GameObjects) to an animation trigger (`Pick`, `Turn`, `Place`, `Home`) on the robot arm's `Animator`, (2) log each activation to the console, and (3) change the status light color based on whether the robot is idle or moving.

1. **Setup the Animator Triggers:**
  - Go to the robot arm's `Animator Controller`.
  - Add or verify states: `Idle`, `Pick`, `Turn`, `Place`.
  - Add or verify transitions using `Trigger Parameters`: `"Home"`, `"Pick"`, `"Turn"`, `"Place"`.

    ![02](/Figures/D5/02.jpg)

2. **Create the `RobotController` Script:**
  - Prepare a `RobotController.cs` script to manage the **robot arm’s animation triggers** and updates a **status light** to reflect whether the robot is active (moving) or idle. A script can help synchronize visual feedback with animation states in a modular, event-driven way.

    ```csharp
    using UnityEngine;

    public class RobotController : MonoBehaviour
    {
        public Animator robotAnimator;
        public GameObject statusLight;
        public Material greenMaterial;
        public Material redMaterial;

        private Renderer statusRenderer;

        // Name of the idle state as it appears in the Animator
        private const string idleStateName = "Robot Arm Idle";

        private void Start()
        {
            if (statusLight != null)
                statusRenderer = statusLight.GetComponent<Renderer>();
        }

        private void Update()
        {
            if (robotAnimator == null || statusRenderer == null) return;

            bool isInTransition = robotAnimator.IsInTransition(0);
            AnimatorStateInfo currentState = robotAnimator.GetCurrentAnimatorStateInfo(0);

            // Light should only be green if not in transition and fully in the idle state
            bool isTrulyIdle = !isInTransition && currentState.IsName(idleStateName);

            statusRenderer.material = isTrulyIdle ? greenMaterial : redMaterial;
        }

        public void TriggerAnimation(string triggerName)
        {
            if (robotAnimator == null) return;

            robotAnimator.SetTrigger(triggerName);
            Debug.Log($"Robot Triggered: {triggerName}");
            // Status light will update automatically in Update()
        }
    }
    ```
3. **Configure the Script:**
  - Attach the script to a GameObject representing the robot (e.g., `Heavy Payload Robot Arm`).
  - `Robot Animator`: Assign the `Animator` component that controls the robot's state machine.
  - `Status Light`: Assign `Status Light`.
  - `Green Material`: Assign a status material to show the robot is idle.
  - `Red Material`: Assign a status material used when the robot is moving.

    ![03](/Figures/D5/03.jpg)

4. **Configure the Numpad Buttons:**
  - From each numpad button, locate the `OnClick()` event,
  - Click the `+` to add a new event.
  - Drag the robot GameObject (with the `RobotController.cs` script) into the field.
  - Select `RobotController → TriggerAnimation(string)`.
  - Enter the corresponding animation trigger in the string field (e.g., `Home`, `Pick`, `Turn`, or `Place`).
  - Repeat this process for each numpad button, assigning the desired trigger to each.

    ![04](/Figures/D5/04.jpg)

5. **(Optional) Use a Configurable Button Mapping:**
  - To make the system flexible, create a script that allows users to assign animation triggers to each numpad button using a dictionary. 
  - Update your `RobotController` script as follows:

    ```csharp
    using UnityEngine;
    using System.Collections.Generic;

    public class RobotController : MonoBehaviour
    {
        public Animator robotAnimator;
        public GameObject statusLight;
        public Material greenMaterial;
        public Material redMaterial;

        private Renderer statusRenderer;

        // Name of the idle state as it appears in the Animator
        private const string idleStateName = "Robot Arm Idle";

        // Configurable mapping from button numbers to animation triggers
        public Dictionary<int, string> buttonMappings = new Dictionary<int, string>()
        {
            { 1, "Pick" },
            { 2, "Turn" },
            { 3, "Place" },
            { 4, "Home" }
        };

        private void Start()
        {
            if (statusLight != null)
                statusRenderer = statusLight.GetComponent<Renderer>();
        }

        private void Update()
        {
            if (robotAnimator == null || statusRenderer == null) return;

            bool isInTransition = robotAnimator.IsInTransition(0);
            AnimatorStateInfo currentState = robotAnimator.GetCurrentAnimatorStateInfo(0);

            bool isTrulyIdle = !isInTransition && currentState.IsName(idleStateName);
            statusRenderer.material = isTrulyIdle ? greenMaterial : redMaterial;
        }

        // This method is called from UI or XR button events
        public void OnButtonPressed(int buttonId)
        {
            if (buttonMappings.ContainsKey(buttonId))
            {
                TriggerAnimation(buttonMappings[buttonId]);
            }
            else
            {
                Debug.LogWarning($"Button ID {buttonId} not mapped to any trigger.");
            }
        }

        public void TriggerAnimation(string triggerName)
        {
            if (robotAnimator == null) return;

            robotAnimator.SetTrigger(triggerName);
            Debug.Log($"Robot Triggered: {triggerName}");
            // Status light updates automatically in Update()
        }
    }
    ```

6. **Configure the Script:**
  - Select the button GameObject in the `Hierarchy` (e.g., `1`, `2`, ...).
  - In the `OnClick()` event section, click the `+` button and drag the **robot GameObject** (which has the `RobotController.cs` script attached) into the event field.
  - From the function dropdown, select `RobotController → OnButtonPressed(int)`.
  - Enter the appropriate button number into the parameter field: Button 1 → `1`, Button 2 → `2`, etc.
  - This will call the correct animation trigger for each button based on the dictionary mapping.

    ![05](/Figures/D5/05.jpg)

    > This setup allows you to change button-to-trigger mappings in one place (in the `buttonMappings` dictionary) and easily support **reconfiguration** via menus or UI tools.

7. **Final Result of the Event-Driven Setup:**
  - Each button press on the XR numpad sends a configurable command to the robot.
  - The robot's animation state changes via an `Animator` trigger.
  - A red or green status light reflects active vs. idle states.
  - Console logging supports debugging or monitoring.
  - All interactions are modular, extensible, and VR-ready.

    ![06](/Figures/D5/06.jpg)

---

# Feedback Mechanisms

In interactive VR systems, especially in engineering and industrial use cases, feedback mechanisms provide essential **sensory cues** that help users understand system status, recognize events, and respond appropriately. Feedback mechanisms improve realism, reinforce user confidence, and promote safety.

## Core Concepts

- **Visual Feedback:** Uses changes in light, material color, UI state, or animation to reflect system changes. Helps users see state transitions at a glance and reinforces spatial awareness.
- **Auditory Feedback:** Uses sound cues (e.g., motors starting, beeps, chimes) to reinforce or replace visual signals. Especially useful when visuals are obstructed, ambiguous, or occur outside the user’s field of view.
- **Tactile (Haptic) Feedback:** Uses vibration or force feedback through XR controllers to physically signal events such as activation, collision, error states, or machine startup. Haptics enhance realism and reinforce perception of system interaction.
- **Multi-Modal Feedback:** Combining visual, auditory, and tactile feedback provides redundancy and depth. This ensures clear communication of system status and reduces the risk of misinterpretation, especially in safety-critical engineering tasks.

> In VR-based engineering scenarios, feedback must mirror real-world interactions—e.g., machines emit sounds when moving, warning lights flash during operation, and systems provide end-of-cycle cues.


## Example: Robot Status

At the welding station of XFactory, the heavy payload robot arm gives the user real-time **multi-modal feedback** when a robot action is triggered. It includes (1) **audio feedback**, playing sounds for movement and idle, (2) **visual feedback**, flashing the button that was last pressed, and (3) **haptic feedback**, sending a short vibration to the VR controller when the robot begins to move. Here is how to implement it:

1. **Create a Script:**  
  - Add a `RobotFeedbackManager.cs` script to a central object in your scene (e.g., an empty `RobotFeedbackManager` GameObjet), or to the robot itself (`Heavy Payload Robot Arm`).
  - Copy the following script into your IDE.

    ```csharp
    using UnityEngine;
    using UnityEngine.XR.Interaction.Toolkit;
    using UnityEngine.UI;
    using System.Collections;

    public class RobotFeedbackManager : MonoBehaviour
    {
        [Header("Audio Feedback")]
        public AudioSource audioSource;
        public AudioClip motorHumClip;
        public AudioClip idleBeepClip;

        [Header("Visual Button Feedback")]
        public float flashDuration = 0.5f;
        public Color flashColor = Color.yellow;
        public float flashInterval = 0.1f;

        [Header("Haptic Feedback")]
        public XRBaseController xrController;
        public float hapticAmplitude = 0.7f;
        public float hapticDuration = 0.2f;

        private Coroutine flashRoutine;

        public void OnRobotStartMoving()
        {
            PlaySound(motorHumClip);
            SendHaptics();
        }

        public void OnRobotReturnToIdle()
        {
            PlaySound(idleBeepClip);
        }

        public void FlashButton(GameObject button)
        {
            if (button == null) return;

            // First, try UI button (Image-based)
            Image img = button.GetComponent<Image>();
            if (img != null)
            {
                if (flashRoutine != null)
                    StopCoroutine(flashRoutine);

                flashRoutine = StartCoroutine(FlashUIRoutine(img));
                return;
            }

            // Fallback to 3D mesh button (Renderer-based)
            Renderer rend = button.GetComponent<Renderer>();
            if (rend != null)
            {
                if (flashRoutine != null)
                    StopCoroutine(flashRoutine);

                flashRoutine = StartCoroutine(Flash3DRoutine(rend));
            }
        }

        private IEnumerator FlashUIRoutine(Image img)
        {
            Color originalColor = img.color;
            float elapsed = 0f;

            while (elapsed < flashDuration)
            {
                img.color = flashColor;
                yield return new WaitForSeconds(flashInterval);
                elapsed += flashInterval;

                img.color = originalColor;
                yield return new WaitForSeconds(flashInterval);
                elapsed += flashInterval;
            }

            img.color = originalColor;
            flashRoutine = null;
        }

        private IEnumerator Flash3DRoutine(Renderer rend)
        {
            Material mat = rend.material; // Safe instance
            Color originalColor = mat.color;
            float elapsed = 0f;

            while (elapsed < flashDuration)
            {
                mat.color = flashColor;
                yield return new WaitForSeconds(flashInterval);
                elapsed += flashInterval;

                mat.color = originalColor;
                yield return new WaitForSeconds(flashInterval);
                elapsed += flashInterval;
            }

            mat.color = originalColor;
            flashRoutine = null;
        }

        private void PlaySound(AudioClip clip)
        {
            if (audioSource != null && clip != null)
            {
                audioSource.Stop();
                audioSource.clip = clip;
                audioSource.Play();
            }
        }

        private void SendHaptics()
        {
            if (xrController != null)
            {
                xrController.SendHapticImpulse(hapticAmplitude, hapticDuration);
            }
        }
    }
    ```

2. **Configure the Script:**
  - `Audio Source`: Assign an `AudioSource` component. Leave the "Audio Clip" field in the AudioSource blank. The script will dynamically assign the appropriate clip at runtime.
  - `Motor Hum Clip`: Assign a sound to play when the robot starts moving (e.g., mechanical hum or activation tone).
  - `Idle Beep Clip`: Assign a sound to play when the robot returns to idle (e.g., soft beep or chime).
  - `Flash Duration`: Set how long the button should flash overall (e.g., `3` seconds).
  - `Flash Color`: Choose the color that the button should blink (typically yellow for feedback).
  - `Flash Interval`: Set how fast the button should blink (e.g., `0.2` seconds per on/off cycle).
  - `XR Controller`: Drag in the `LeftHand Controller` or `RightHand Controller` GameObject from your `XR Rig`.  

    ![07](/Figures/D5/07.jpg)

3. **Trigger Feedback:**
  - Select a numpad button GameObject in the `Hierarchy` (e.g., `1`, `2`, etc.).
  - In the `Button (Script)` component, go to the `OnClick()` event list.
  - Click the `+` button to add a second event.
  - Drag the robot GameObject (`Heavy Payload Robot Arm`) into the event slot.
  - From the dropdown, select `RobotFeedbackManager → OnRobotStartMoving()` (this enables auditory feedback).
  - Click `+` again to add a third event.
  - Drag the robot GameObject into this slot.
  - From the dropdown, choose `RobotFeedbackManager → FlashButton(GameObject)` (this enables visual feedback).
  - Drag the `same button GameObject` into the argument field.
  - Repeat the above for all numpad buttons.

    ![08](/Figures/D5/08.jpg)

4. **Specify Idle State at the End of Animation:**
  - In the `Animator`, go to the end of your `Robot Arm Idle` animation.
  - Add an `Animation Event` to the last frame.
  - Set the function to `OnRobotReturnToIdle`.

    ![09](/Figures/D5/09.jpg)

5. **Outcome:**
  - When the robot is activated (e.g., via numpad button), the status light turns red, a motor hum sound plays, and the user's **controller vibrates** (haptic pulse).
  - When the robot returns to idle (triggered by animation event at the end of the idle animation), the status light turns green, and a beep (or idle motor) sound plays

> This feedback loop simulates a real-world factory machine's behavioral cues, enhancing **immersion**, **safety**, and **real-time situational awareness** for learners.

---

# Contextual Interactions

**Contextual Interactions** are interactions that adapt based on the current **state** or **condition** of a system. In XR use cases for engineering, such interactions are essential to simulate real-world safety protocols, machine readiness, or task sequencing.

## Core Concepts

- **System State Awareness:** Interactions should only be allowed when the system is in a valid state (e.g., safe, initialized, not in error).
- **State-Dependent Behavior:** Actions may be blocked, delayed, or altered based on safety flags, mode selection, or operational status.
- **Safety by Design:** Simulating restrictions (e.g., e-stop, lockouts, interlocks) promotes realism and teaches proper industrial behavior.
- **Feedback on Invalid Interactions:** The system should inform the user *why* an action is blocked—often through lights, sounds, or notifications.


## Example: Robot E-Stop

At the welding station of XFactory, an **emergency stop button** governs the entire robot operation. If the e-stop is engaged, the **numpad is disabled**, any button press triggers a **buzz alarm** and a **flashing red status light**, and robot animations are blocked to simulate faulty state. If the e-stop is **cleared**, however, normal operation resumes, the light returns to **solid green**, sounds are reset and input is re-enabled. Here is how to implement this behavior:

1. **Create a System Manager Script:**
  - Create a new empty GameObject (`System Manager`) as a child of `Heavy Payload Robot Arm`. 
  - Attach the following script to `System Manager` to manage the global system state and toggles interaction accordingly.

    ```csharp
    using UnityEngine;
    using UnityEngine.Events;

    public class SystemStateManager : MonoBehaviour
    {
        public bool isEmergencyStopped = false;
        public UnityEvent onEStopEngaged;
        public UnityEvent onEStopCleared;

        public void ToggleEmergencyStop()
        {
            isEmergencyStopped = !isEmergencyStopped;
            if (isEmergencyStopped)
            {
                Debug.Log("Emergency Stop Engaged");
                onEStopEngaged?.Invoke();
            }
            else
            {
                Debug.Log("Emergency Stop Cleared");
                onEStopCleared?.Invoke();
            }
        }
    }
    ```

2. **Configure the Script in the `Inspector`:**
  - Leave `isEmergencyStopped` unchecked (default is normal state).
  - Under `onEStopEngaged` (when e-stop is pressed), click the `+` button. Add each numpad button individually. Assign `Button → bool interactable` and **uncheck** the box to disable interaction during emergency stop.
  - Under `onEStopCleared` (when e-stop is reset), click the `+` button again. Add the same objects. Assign `Button → bool interactable` and **check** the box to re-enable interaction when normal operation resumes.

    ![10](/Figures/D5/10.jpg)


3. **Update `RobotController.cs` to Check State:**
  -Modify your `RobotController.cs` to respect the emergency stop:

    ```csharp
    using UnityEngine;

    public class RobotController : MonoBehaviour
    {
        public Animator robotAnimator;
        public GameObject statusLight;
        public Material greenMaterial;
        public Material redMaterial;
        public SystemStateManager systemStateManager;
        public RobotFeedbackManager feedbackManager;

        private Renderer statusRenderer;

        // Name of the idle state as it appears in the Animator
        private const string idleStateName = "Robot Arm Idle";

        private void Start()
        {
            if (statusLight != null)
                statusRenderer = statusLight.GetComponent<Renderer>();
        }

        private void Update()
        {
            if (robotAnimator == null || statusRenderer == null) return;

            bool isInTransition = robotAnimator.IsInTransition(0);
            AnimatorStateInfo currentState = robotAnimator.GetCurrentAnimatorStateInfo(0);

            // Light should only be green if not in transition and fully in the idle state
            bool isTrulyIdle = !isInTransition && currentState.IsName(idleStateName);

            statusRenderer.material = isTrulyIdle ? greenMaterial : redMaterial;
        }

        public void TriggerAnimation(string triggerName)
        {
            if (systemStateManager != null && systemStateManager.isEmergencyStopped)
            {
                Debug.Log("Blocked: System is in Emergency Stop");
                feedbackManager?.OnEmergencyBlocked(); // Optional: handle blocked input feedback
                return;
            }

            if (robotAnimator == null) return;

            robotAnimator.SetTrigger(triggerName);
            feedbackManager?.OnRobotStartMoving();
            Debug.Log($"Robot Triggered: {triggerName}");
        }
    }
    ```

4. **Configure the Script:**
  - Select the robot GameObject with `RobotController.cs`.
  - In the `System State Manager` field, drag in the `System Manager` GameObject (the one with the `SystemStateManager.cs` script).
  - In the `Feedback Manager` field, assign the robot itself (which contains the `RobotFeedbackManager.cs` script).

    ![11](/Figures/D5/11.jpg)

5. **Add Emergency Feedback to `FeedbackManager.cs`:**
  - Expand the `RobotFeedbackManager.cs` to support flashing and buzzing.

    ```csharp
    private Coroutine flashingCoroutine;

    public AudioClip errorBuzzClip;

    public void OnEmergencyBlocked()
    {
        StartFlashingRed();
        PlaySound(errorBuzzClip);
    }

    public void OnEmergencyCleared()
    {
        StopFlashing();
        SetLightColor(greenMaterial);
        audioSource.Stop();
    }

    private void StartFlashingRed()
    {
        if (flashingCoroutine != null) return;
        flashingCoroutine = StartCoroutine(FlashRed());
    }

    private void StopFlashing()
    {
        if (flashingCoroutine != null)
        {
            StopCoroutine(flashingCoroutine);
            flashingCoroutine = null;
        }
    }

    private IEnumerator FlashRed()
    {
        while (true)
        {
            statusLightRenderer.material = redMaterial;
            yield return new WaitForSeconds(0.5f);
            statusLightRenderer.material = greenMaterial;
            yield return new WaitForSeconds(0.5f);
        }
    }
    ```

6. **Configure the Script:**
  - `Error Buzz Clip`: Assign a short alert/buzzer audio clip.
  - `Status Light Renderer`: Drag in the `Renderer` component of the robot’s status light (`Status Light`).
  - `Red Material`: Assign a red material used to indicate emergency.
  - `Green Material`: Assign a green material used to indicate normal operation.

    ![12](/Figures/D5/12.jpg)

7. **Hook Up the Emergency Stop Button:**
  - In the `E-Stop` button’s `Inspector`, locate the `OnClick()` event section.
  - Click the `+` to add a new event.
  - Drag the `System Manager` GameObject (the one with `SystemStateManager.cs`) into the object field.
  - From the dropdown, select `SystemStateManager → ToggleEmergencyStop()`. This enables toggling the emergency state on and off when the e-stop is pressed.

    ![13](/Figures/D5/13.jpg)

8. **Wire Unity Events in the `Inspector`:**
  - Use Unity Events to link state transitions to visual and audio feedback.
  - Locate the `SystemStateManager` component `On E Stop Engaged`. Drag `Heavy Payload Robot Arm` into the field and call `RobotFeedbackManager.OnEmergencyBlocked()`.
  - Similarly, locate `On E Stop Cleared` and call `RobotFeedbackManager.OnEmergencyCleared()`

    ![14](/Figures/D5/14.jpg)

9. **Outcome:**
  - Robot control is disabled during an emergency stop.
  - Pressing any number on the numpad while the e-stop is active results in a **flashing red status light**, a **buzz alarm sound**, and no animation is triggered on the robot.
  - Releasing the e-stop (pressing again) returns the system to normal. The status light turns **solid green**, the **buzzing stops**, the numpad becomes **interactive** again, and robot controls are **re-enabled**.

> This reflects **real-world industrial safety behavior**, where all interactive systems respect current operational context, promoting safety, realism, and reliable user feedback.